{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = load_boston(return_X_y=True)\n",
    "y = y[..., np.newaxis]\n",
    "y = np.concatenate((y, y * 2), axis=-1)\n",
    "X = StandardScaler().fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = torch.ones(13, 2)\n",
    "b = torch.zeros(2)\n",
    "X_t = torch.from_numpy(X).float()\n",
    "y_t = torch.from_numpy(y).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1554.0861, grad_fn=<DivBackward0>)\n",
      "tensor(55.4702, grad_fn=<DivBackward0>)\n",
      "tensor(54.8817, grad_fn=<DivBackward0>)\n",
      "tensor(54.7757, grad_fn=<DivBackward0>)\n",
      "tensor(54.7478, grad_fn=<DivBackward0>)\n",
      "tensor(54.7401, grad_fn=<DivBackward0>)\n",
      "tensor(54.7379, grad_fn=<DivBackward0>)\n",
      "tensor(54.7373, grad_fn=<DivBackward0>)\n",
      "tensor(54.7372, grad_fn=<DivBackward0>)\n",
      "tensor(54.7371, grad_fn=<DivBackward0>)\n",
      "tensor(54.7371, grad_fn=<DivBackward0>)\n",
      "tensor(54.7371, grad_fn=<DivBackward0>)\n",
      "tensor(54.7371, grad_fn=<DivBackward0>)\n",
      "tensor(54.7371, grad_fn=<DivBackward0>)\n",
      "tensor(54.7371, grad_fn=<DivBackward0>)\n",
      "tensor(54.7371, grad_fn=<DivBackward0>)\n",
      "tensor(54.7371, grad_fn=<DivBackward0>)\n",
      "tensor(54.7371, grad_fn=<DivBackward0>)\n",
      "tensor(54.7371, grad_fn=<DivBackward0>)\n",
      "tensor(54.7371, grad_fn=<DivBackward0>)\n",
      "tensor(54.7371, grad_fn=<DivBackward0>)\n",
      "tensor(54.7371, grad_fn=<DivBackward0>)\n",
      "tensor(54.7371, grad_fn=<DivBackward0>)\n",
      "tensor(54.7371, grad_fn=<DivBackward0>)\n",
      "tensor(54.7371, grad_fn=<DivBackward0>)\n",
      "tensor(54.7371, grad_fn=<DivBackward0>)\n",
      "tensor(54.7371, grad_fn=<DivBackward0>)\n",
      "tensor(54.7371, grad_fn=<DivBackward0>)\n",
      "tensor(54.7371, grad_fn=<DivBackward0>)\n",
      "tensor(54.7371, grad_fn=<DivBackward0>)\n",
      "tensor(54.7371, grad_fn=<DivBackward0>)\n",
      "tensor(54.7371, grad_fn=<DivBackward0>)\n",
      "tensor(54.7371, grad_fn=<DivBackward0>)\n",
      "tensor(54.7371, grad_fn=<DivBackward0>)\n",
      "tensor(54.7371, grad_fn=<DivBackward0>)\n",
      "tensor(54.7371, grad_fn=<DivBackward0>)\n",
      "tensor(54.7371, grad_fn=<DivBackward0>)\n",
      "tensor(54.7371, grad_fn=<DivBackward0>)\n",
      "tensor(54.7371, grad_fn=<DivBackward0>)\n",
      "tensor(54.7371, grad_fn=<DivBackward0>)\n",
      "tensor(54.7371, grad_fn=<DivBackward0>)\n",
      "tensor(54.7371, grad_fn=<DivBackward0>)\n",
      "tensor(54.7371, grad_fn=<DivBackward0>)\n",
      "tensor(54.7371, grad_fn=<DivBackward0>)\n",
      "tensor(54.7371, grad_fn=<DivBackward0>)\n",
      "tensor(54.7371, grad_fn=<DivBackward0>)\n",
      "tensor(54.7371, grad_fn=<DivBackward0>)\n",
      "tensor(54.7371, grad_fn=<DivBackward0>)\n",
      "tensor(54.7371, grad_fn=<DivBackward0>)\n",
      "tensor(54.7371, grad_fn=<DivBackward0>)\n",
      "tensor(54.7371, grad_fn=<DivBackward0>)\n",
      "tensor(54.7371, grad_fn=<DivBackward0>)\n",
      "tensor(54.7371, grad_fn=<DivBackward0>)\n",
      "tensor(54.7371, grad_fn=<DivBackward0>)\n",
      "tensor(54.7371, grad_fn=<DivBackward0>)\n",
      "tensor(54.7371, grad_fn=<DivBackward0>)\n",
      "tensor(54.7371, grad_fn=<DivBackward0>)\n",
      "tensor(54.7371, grad_fn=<DivBackward0>)\n",
      "tensor(54.7371, grad_fn=<DivBackward0>)\n",
      "tensor(54.7371, grad_fn=<DivBackward0>)\n",
      "tensor(54.7371, grad_fn=<DivBackward0>)\n",
      "tensor(54.7371, grad_fn=<DivBackward0>)\n",
      "tensor(54.7371, grad_fn=<DivBackward0>)\n",
      "tensor(54.7371, grad_fn=<DivBackward0>)\n",
      "tensor(54.7371, grad_fn=<DivBackward0>)\n",
      "tensor(54.7371, grad_fn=<DivBackward0>)\n",
      "tensor(54.7371, grad_fn=<DivBackward0>)\n",
      "tensor(54.7371, grad_fn=<DivBackward0>)\n",
      "tensor(54.7371, grad_fn=<DivBackward0>)\n",
      "tensor(54.7371, grad_fn=<DivBackward0>)\n",
      "tensor(54.7371, grad_fn=<DivBackward0>)\n",
      "tensor(54.7371, grad_fn=<DivBackward0>)\n",
      "tensor(54.7371, grad_fn=<DivBackward0>)\n",
      "tensor(54.7371, grad_fn=<DivBackward0>)\n",
      "tensor(54.7371, grad_fn=<DivBackward0>)\n",
      "tensor(54.7371, grad_fn=<DivBackward0>)\n",
      "tensor(54.7371, grad_fn=<DivBackward0>)\n",
      "tensor(54.7371, grad_fn=<DivBackward0>)\n",
      "tensor(54.7371, grad_fn=<DivBackward0>)\n",
      "tensor(54.7371, grad_fn=<DivBackward0>)\n",
      "tensor(54.7371, grad_fn=<DivBackward0>)\n",
      "tensor(54.7371, grad_fn=<DivBackward0>)\n",
      "tensor(54.7371, grad_fn=<DivBackward0>)\n",
      "tensor(54.7371, grad_fn=<DivBackward0>)\n",
      "tensor(54.7371, grad_fn=<DivBackward0>)\n",
      "tensor(54.7371, grad_fn=<DivBackward0>)\n",
      "tensor(54.7371, grad_fn=<DivBackward0>)\n",
      "tensor(54.7371, grad_fn=<DivBackward0>)\n",
      "tensor(54.7371, grad_fn=<DivBackward0>)\n",
      "tensor(54.7371, grad_fn=<DivBackward0>)\n",
      "tensor(54.7371, grad_fn=<DivBackward0>)\n",
      "tensor(54.7371, grad_fn=<DivBackward0>)\n",
      "tensor(54.7371, grad_fn=<DivBackward0>)\n",
      "tensor(54.7371, grad_fn=<DivBackward0>)\n",
      "tensor(54.7371, grad_fn=<DivBackward0>)\n",
      "tensor(54.7371, grad_fn=<DivBackward0>)\n",
      "tensor(54.7371, grad_fn=<DivBackward0>)\n",
      "tensor(54.7371, grad_fn=<DivBackward0>)\n",
      "tensor(54.7371, grad_fn=<DivBackward0>)\n",
      "tensor(54.7371, grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for i in range(10000):\n",
    "    \n",
    "    w.requires_grad = True\n",
    "    b.requires_grad = True\n",
    "    \n",
    "    y__t = X_t @ w + b\n",
    "    \n",
    "#     y__t.requires_grad = True\n",
    "    \n",
    "    \n",
    "    assert y__t.shape == y_t.shape\n",
    "        \n",
    "    l = 0.5 * torch.sum((y_t - y__t) ** 2) / len(y_t)\n",
    "    l.backward()\n",
    "    if i % 100 == 0:\n",
    "        print(l)\n",
    "#     print(b.grad)\n",
    "    w = w.data - 0.1 * w.grad\n",
    "    b = b.data - 0.1 * b.grad\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear:\n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        self.w = np.ones([in_dim, out_dim])\n",
    "#         self.w = np.random.randn(in_dim, out_dim)\n",
    "        self.b = np.zeros(out_dim)\n",
    "        self.dw = None\n",
    "        self.db = None\n",
    "        self.in_dim = self.w.shape[0]\n",
    "        self.out_dim = self.w.shape[1]\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "        return np.matmul(x, self.w) + self.b\n",
    "    \n",
    "    def backward(self, d):\n",
    "        \n",
    "        J = np.zeros([self.x.shape[0], self.out_dim, np.prod(self.w.shape)])\n",
    "        j = 0\n",
    "        for i in range(self.out_dim):\n",
    "            J[:, i: i + 1, j: j + self.in_dim] = self.x[:, np.newaxis, :]\n",
    "            j += self.in_dim\n",
    "        \n",
    "        \n",
    "        self.db = np.mean(d, 0)\n",
    "        assert self.db.shape == self.b.shape, (d.shape, self.db.shape, self.b.shape)\n",
    "        \n",
    "        d = d[:, :, np.newaxis]\n",
    "        J = np.swapaxes(J, -2, -1)\n",
    "\n",
    "        self.dw = np.reshape(np.mean(J @ d, axis=0), self.w.shape, order='F')\n",
    "        assert self.dw.shape == self.w.shape, (self.dw.shape, self.w.shape)\n",
    "                \n",
    "#         J = np.repeat(self.w[np.newaxis, ...], self.x.shape[0], axis=0)\n",
    "#         d = J @ d.T\n",
    "        return d        \n",
    "        \n",
    "    def step(self, lr):\n",
    "        self.w = self.w - lr * self.dw\n",
    "        self.b = self.b - lr * self.db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MeanSquaredError:\n",
    "    def __init__(self):\n",
    "        self.y_ = None\n",
    "    def forward(self, y_, y):\n",
    "        assert y_.shape == y.shape\n",
    "        self.y_ = y_\n",
    "        self.y = y\n",
    "#         print(y_)\n",
    "        l = 0.5 * np.sum(np.square(y - y_)) / len(y)\n",
    "        return l\n",
    "    \n",
    "    def backward(self):\n",
    "        d = -(self.y - self.y_)\n",
    "        return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Linear:\n",
    "#     def __init__(self, in_dim, out_dim):\n",
    "#         self.w = np.random.randn(in_dim, out_dim)\n",
    "#         self.b = np.zeros([out_dim])\n",
    "#         self.dw = None\n",
    "#         self.db = None\n",
    "#         self.x = None\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         self.x = x\n",
    "#         return np.matmul(x, self.w) + self.b\n",
    "    \n",
    "#     def backward(self, d):\n",
    "#         self.dw = np.matmul(self.x.T, d)\n",
    "#         self.db = np.sum(d, axis=0)\n",
    "#         assert self.dw.shape == self.w.shape\n",
    "#         assert self.db.shape == self.b.shape\n",
    "\n",
    "#         d = d @ self.w.T\n",
    "#         return d\n",
    "        \n",
    "#     def step(self, lr):\n",
    "#         self.w = self.w - lr * self.dw\n",
    "#         self.b = self.b - lr * self.db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegression:\n",
    "    def __init__(self):\n",
    "        self.linear = Linear(13, 2)\n",
    "        self.loss = MeanSquaredError()\n",
    "        \n",
    "    def forward(self, x, y):\n",
    "        x = self.linear.forward(x)\n",
    "        loss = self.loss.forward(x, y)\n",
    "        return loss\n",
    "    \n",
    "    def backward(self):\n",
    "        d = self.loss.backward()\n",
    "        self.linear.backward(d)\n",
    "    \n",
    "    def step(self, lr):\n",
    "        self.linear.step(lr)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = np.array([[1, 2, 3], [1, 2, 3]])\n",
    "# y = np.array([[4], [4]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "linreg = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1554.0859822096998 loss\n",
      "55.470173610926885 loss\n",
      "54.88169658632446 loss\n",
      "54.77571618031153 loss\n",
      "54.74782663407481 loss\n",
      "54.74008197277605 loss\n",
      "54.73791795418272 loss\n",
      "54.737312853873746 loss\n",
      "54.73714364265141 loss\n",
      "54.737096323706865 loss\n",
      "54.73708309122181 loss\n",
      "54.73707939082898 loss\n",
      "54.73707835603414 loss\n",
      "54.73707806665936 loss\n",
      "54.73707798573726 loss\n",
      "54.737077963107836 loss\n",
      "54.737077956779636 loss\n",
      "54.73707795500999 loss\n",
      "54.73707795451513 loss\n",
      "54.73707795437673 loss\n",
      "54.73707795433804 loss\n",
      "54.73707795432722 loss\n",
      "54.737077954324185 loss\n",
      "54.73707795432334 loss\n",
      "54.7370779543231 loss\n",
      "54.737077954323034 loss\n",
      "54.73707795432302 loss\n",
      "54.737077954323006 loss\n",
      "54.737077954323006 loss\n",
      "54.737077954323006 loss\n",
      "54.73707795432302 loss\n",
      "54.737077954323006 loss\n",
      "54.73707795432301 loss\n",
      "54.737077954323006 loss\n",
      "54.737077954323006 loss\n",
      "54.737077954323006 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.737077954323006 loss\n",
      "54.737077954323006 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.737077954323006 loss\n",
      "54.737077954323006 loss\n",
      "54.737077954323006 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.737077954323006 loss\n",
      "54.737077954323006 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n",
      "54.73707795432301 loss\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/Users/david/miniconda3/envs/py37/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3331, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-11-bf5140d92fe1>\", line 5, in <module>\n",
      "    linreg.backward()\n",
      "  File \"<ipython-input-8-d5bc70f4b7ee>\", line 13, in backward\n",
      "    self.linear.backward(d)\n",
      "  File \"<ipython-input-5-40a92187aedc>\", line 21, in backward\n",
      "    J[:, i: i + 1, j: j + self.in_dim] = self.x[:, np.newaxis, :]\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/david/miniconda3/envs/py37/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 2044, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/david/miniconda3/envs/py37/lib/python3.7/site-packages/IPython/core/ultratb.py\", line 1148, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"/Users/david/miniconda3/envs/py37/lib/python3.7/site-packages/IPython/core/ultratb.py\", line 316, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/Users/david/miniconda3/envs/py37/lib/python3.7/site-packages/IPython/core/ultratb.py\", line 350, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"/Users/david/miniconda3/envs/py37/lib/python3.7/inspect.py\", line 1502, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"/Users/david/miniconda3/envs/py37/lib/python3.7/inspect.py\", line 1460, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"/Users/david/miniconda3/envs/py37/lib/python3.7/inspect.py\", line 696, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"/Users/david/miniconda3/envs/py37/lib/python3.7/inspect.py\", line 742, in getmodule\n",
      "    os.path.realpath(f)] = module.__name__\n",
      "  File \"/Users/david/miniconda3/envs/py37/lib/python3.7/posixpath.py\", line 395, in realpath\n",
      "    path, ok = _joinrealpath(filename[:0], filename, {})\n",
      "  File \"/Users/david/miniconda3/envs/py37/lib/python3.7/posixpath.py\", line 429, in _joinrealpath\n",
      "    if not islink(newpath):\n",
      "  File \"/Users/david/miniconda3/envs/py37/lib/python3.7/posixpath.py\", line 171, in islink\n",
      "    st = os.lstat(path)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m"
     ]
    }
   ],
   "source": [
    "for i in range(100000):\n",
    "    loss = linreg.forward(X, y)\n",
    "    if i % 100 == 0:\n",
    "        print(loss, 'loss')\n",
    "    linreg.backward()\n",
    "    linreg.step(0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(0.5 * (lr.predict(X) - y) ** 2) / len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linreg.linear.forward(X).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr.predict(X).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
