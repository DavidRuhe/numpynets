{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch\n",
    "import os\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "\n",
    "if os.path.exists('X.npy') and os.path.exists('y.npy'):\n",
    "    X = np.load('X.npy', allow_pickle=True)\n",
    "    y = np.load('y.npy', allow_pickle=True)\n",
    "else:\n",
    "    X, y = fetch_openml('mnist_784', version=1, return_X_y=True)\n",
    "    np.save('X.npy', X)\n",
    "    np.save('y.npy', y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X, y = load_digits(return_X_y=True)\n",
    "\n",
    "y = np.eye(10)[y.astype(np.int32)].astype(np.float32)\n",
    "X = StandardScaler().fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((70000, 784), (70000, 10))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = 784"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = shuffle(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1 = torch.repeat_interleave(torch.linspace(-0.1, 0.1, 32)[None, ...], num_features, dim=0)\n",
    "w2 = torch.repeat_interleave(torch.linspace(-0.1, 0.1, 10)[None, ...], 32, dim=0)\n",
    "\n",
    "# w1 = torch.randn(73, 32) * 0.01\n",
    "# w2 = torch.randn(32, 10) * 0.01\n",
    "\n",
    "b1 = torch.zeros(32)\n",
    "b2 = torch.zeros(10)\n",
    "\n",
    "X_t = torch.from_numpy(X).float()\n",
    "y_t = torch.from_numpy(y).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((70000, 784), (70000, 10))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(8.4004, grad_fn=<DivBackward0>)\n",
      "tensor(0.1104)\n",
      "tensor(7.0642, grad_fn=<DivBackward0>)\n",
      "tensor(0.1172)\n",
      "tensor(6.3599, grad_fn=<DivBackward0>)\n",
      "tensor(0.1299)\n",
      "tensor(5.7995, grad_fn=<DivBackward0>)\n",
      "tensor(0.1357)\n",
      "tensor(5.1563, grad_fn=<DivBackward0>)\n",
      "tensor(0.1250)\n",
      "tensor(4.5555, grad_fn=<DivBackward0>)\n",
      "tensor(0.1260)\n",
      "tensor(4.3211, grad_fn=<DivBackward0>)\n",
      "tensor(0.1367)\n",
      "tensor(3.9158, grad_fn=<DivBackward0>)\n",
      "tensor(0.1504)\n",
      "tensor(3.5749, grad_fn=<DivBackward0>)\n",
      "tensor(0.1240)\n",
      "tensor(3.3205, grad_fn=<DivBackward0>)\n",
      "tensor(0.1201)\n",
      "tensor(2.9268, grad_fn=<DivBackward0>)\n",
      "tensor(0.1162)\n",
      "tensor(2.6954, grad_fn=<DivBackward0>)\n",
      "tensor(0.1836)\n",
      "tensor(2.5745, grad_fn=<DivBackward0>)\n",
      "tensor(0.1719)\n",
      "tensor(2.5182, grad_fn=<DivBackward0>)\n",
      "tensor(0.1787)\n",
      "tensor(2.3609, grad_fn=<DivBackward0>)\n",
      "tensor(0.1787)\n",
      "tensor(2.3237, grad_fn=<DivBackward0>)\n",
      "tensor(0.1943)\n",
      "tensor(2.2600, grad_fn=<DivBackward0>)\n",
      "tensor(0.2197)\n",
      "tensor(2.1891, grad_fn=<DivBackward0>)\n",
      "tensor(0.1982)\n",
      "tensor(2.1719, grad_fn=<DivBackward0>)\n",
      "tensor(0.1992)\n",
      "tensor(2.1471, grad_fn=<DivBackward0>)\n",
      "tensor(0.2207)\n",
      "tensor(2.1418, grad_fn=<DivBackward0>)\n",
      "tensor(0.2080)\n",
      "tensor(2.1948, grad_fn=<DivBackward0>)\n",
      "tensor(0.1992)\n",
      "tensor(2.1694, grad_fn=<DivBackward0>)\n",
      "tensor(0.2256)\n",
      "tensor(2.1299, grad_fn=<DivBackward0>)\n",
      "tensor(0.2041)\n",
      "tensor(2.1425, grad_fn=<DivBackward0>)\n",
      "tensor(0.2197)\n",
      "tensor(2.1588, grad_fn=<DivBackward0>)\n",
      "tensor(0.1904)\n",
      "tensor(2.1548, grad_fn=<DivBackward0>)\n",
      "tensor(0.2061)\n",
      "tensor(2.1450, grad_fn=<DivBackward0>)\n",
      "tensor(0.1904)\n",
      "tensor(2.1250, grad_fn=<DivBackward0>)\n",
      "tensor(0.2266)\n",
      "tensor(2.1450, grad_fn=<DivBackward0>)\n",
      "tensor(0.1934)\n",
      "tensor(2.1083, grad_fn=<DivBackward0>)\n",
      "tensor(0.2002)\n",
      "tensor(2.1110, grad_fn=<DivBackward0>)\n",
      "tensor(0.2227)\n",
      "tensor(2.1091, grad_fn=<DivBackward0>)\n",
      "tensor(0.2061)\n",
      "tensor(2.1346, grad_fn=<DivBackward0>)\n",
      "tensor(0.2070)\n",
      "tensor(2.0850, grad_fn=<DivBackward0>)\n",
      "tensor(0.2207)\n",
      "tensor(2.1112, grad_fn=<DivBackward0>)\n",
      "tensor(0.2158)\n",
      "tensor(2.0785, grad_fn=<DivBackward0>)\n",
      "tensor(0.2236)\n",
      "tensor(2.1192, grad_fn=<DivBackward0>)\n",
      "tensor(0.2109)\n",
      "tensor(2.0945, grad_fn=<DivBackward0>)\n",
      "tensor(0.2129)\n",
      "tensor(2.1526, grad_fn=<DivBackward0>)\n",
      "tensor(0.2139)\n",
      "tensor(2.1288, grad_fn=<DivBackward0>)\n",
      "tensor(0.2129)\n",
      "tensor(2.0864, grad_fn=<DivBackward0>)\n",
      "tensor(0.2256)\n",
      "tensor(2.0929, grad_fn=<DivBackward0>)\n",
      "tensor(0.2236)\n",
      "tensor(2.1208, grad_fn=<DivBackward0>)\n",
      "tensor(0.2070)\n",
      "tensor(2.0906, grad_fn=<DivBackward0>)\n",
      "tensor(0.2285)\n",
      "tensor(2.1460, grad_fn=<DivBackward0>)\n",
      "tensor(0.2070)\n",
      "tensor(2.1224, grad_fn=<DivBackward0>)\n",
      "tensor(0.2236)\n",
      "tensor(2.1016, grad_fn=<DivBackward0>)\n",
      "tensor(0.1943)\n",
      "tensor(2.0985, grad_fn=<DivBackward0>)\n",
      "tensor(0.2070)\n",
      "tensor(2.0960, grad_fn=<DivBackward0>)\n",
      "tensor(0.2207)\n",
      "tensor(2.0317, grad_fn=<DivBackward0>)\n",
      "tensor(0.2256)\n",
      "tensor(2.0237, grad_fn=<DivBackward0>)\n",
      "tensor(0.2666)\n",
      "tensor(2.1089, grad_fn=<DivBackward0>)\n",
      "tensor(0.2236)\n",
      "tensor(2.0228, grad_fn=<DivBackward0>)\n",
      "tensor(0.2490)\n",
      "tensor(2.0910, grad_fn=<DivBackward0>)\n",
      "tensor(0.2148)\n",
      "tensor(2.0409, grad_fn=<DivBackward0>)\n",
      "tensor(0.2471)\n",
      "tensor(2.0811, grad_fn=<DivBackward0>)\n",
      "tensor(0.2314)\n",
      "tensor(2.0579, grad_fn=<DivBackward0>)\n",
      "tensor(0.2051)\n",
      "tensor(2.0682, grad_fn=<DivBackward0>)\n",
      "tensor(0.2510)\n",
      "tensor(2.1184, grad_fn=<DivBackward0>)\n",
      "tensor(0.2021)\n",
      "tensor(2.0501, grad_fn=<DivBackward0>)\n",
      "tensor(0.2412)\n",
      "tensor(2.0342, grad_fn=<DivBackward0>)\n",
      "tensor(0.2334)\n",
      "tensor(2.0890, grad_fn=<DivBackward0>)\n",
      "tensor(0.2295)\n",
      "tensor(2.1027, grad_fn=<DivBackward0>)\n",
      "tensor(0.2344)\n",
      "tensor(2.0432, grad_fn=<DivBackward0>)\n",
      "tensor(0.2549)\n",
      "tensor(2.0498, grad_fn=<DivBackward0>)\n",
      "tensor(0.2412)\n",
      "tensor(2.0917, grad_fn=<DivBackward0>)\n",
      "tensor(0.2402)\n",
      "tensor(2.0683, grad_fn=<DivBackward0>)\n",
      "tensor(0.2559)\n",
      "tensor(2.0842, grad_fn=<DivBackward0>)\n",
      "tensor(0.2500)\n",
      "tensor(2.0508, grad_fn=<DivBackward0>)\n",
      "tensor(0.2275)\n",
      "tensor(2.0245, grad_fn=<DivBackward0>)\n",
      "tensor(0.2373)\n",
      "tensor(2.1053, grad_fn=<DivBackward0>)\n",
      "tensor(0.2285)\n",
      "tensor(2.0209, grad_fn=<DivBackward0>)\n",
      "tensor(0.2471)\n",
      "tensor(2.0224, grad_fn=<DivBackward0>)\n",
      "tensor(0.2451)\n",
      "tensor(2.0678, grad_fn=<DivBackward0>)\n",
      "tensor(0.2236)\n",
      "tensor(2.0580, grad_fn=<DivBackward0>)\n",
      "tensor(0.2412)\n",
      "tensor(2.0404, grad_fn=<DivBackward0>)\n",
      "tensor(0.2354)\n",
      "tensor(2.0685, grad_fn=<DivBackward0>)\n",
      "tensor(0.2256)\n",
      "tensor(2.0139, grad_fn=<DivBackward0>)\n",
      "tensor(0.2549)\n",
      "tensor(2.0403, grad_fn=<DivBackward0>)\n",
      "tensor(0.2354)\n",
      "tensor(2.0538, grad_fn=<DivBackward0>)\n",
      "tensor(0.2363)\n",
      "tensor(2.0141, grad_fn=<DivBackward0>)\n",
      "tensor(0.2578)\n",
      "tensor(2.0449, grad_fn=<DivBackward0>)\n",
      "tensor(0.2607)\n",
      "tensor(2.0332, grad_fn=<DivBackward0>)\n",
      "tensor(0.2383)\n",
      "tensor(2.0483, grad_fn=<DivBackward0>)\n",
      "tensor(0.2441)\n",
      "tensor(2.0319, grad_fn=<DivBackward0>)\n",
      "tensor(0.2666)\n",
      "tensor(1.9917, grad_fn=<DivBackward0>)\n",
      "tensor(0.2676)\n",
      "tensor(2.0269, grad_fn=<DivBackward0>)\n",
      "tensor(0.2402)\n",
      "tensor(1.9878, grad_fn=<DivBackward0>)\n",
      "tensor(0.2676)\n",
      "tensor(2.0172, grad_fn=<DivBackward0>)\n",
      "tensor(0.2607)\n",
      "tensor(2.0438, grad_fn=<DivBackward0>)\n",
      "tensor(0.2520)\n",
      "tensor(2.0182, grad_fn=<DivBackward0>)\n",
      "tensor(0.2744)\n",
      "tensor(1.9881, grad_fn=<DivBackward0>)\n",
      "tensor(0.2520)\n",
      "tensor(2.0030, grad_fn=<DivBackward0>)\n",
      "tensor(0.2686)\n",
      "tensor(2.0259, grad_fn=<DivBackward0>)\n",
      "tensor(0.2451)\n",
      "tensor(2.0144, grad_fn=<DivBackward0>)\n",
      "tensor(0.2578)\n",
      "tensor(2.0189, grad_fn=<DivBackward0>)\n",
      "tensor(0.2529)\n",
      "tensor(1.9987, grad_fn=<DivBackward0>)\n",
      "tensor(0.2812)\n",
      "tensor(2.0226, grad_fn=<DivBackward0>)\n",
      "tensor(0.2432)\n",
      "tensor(1.9869, grad_fn=<DivBackward0>)\n",
      "tensor(0.2510)\n",
      "tensor(1.9866, grad_fn=<DivBackward0>)\n",
      "tensor(0.2744)\n",
      "tensor(1.9900, grad_fn=<DivBackward0>)\n",
      "tensor(0.2568)\n",
      "tensor(2.0201, grad_fn=<DivBackward0>)\n",
      "tensor(0.2656)\n",
      "tensor(1.9693, grad_fn=<DivBackward0>)\n",
      "tensor(0.2764)\n",
      "tensor(1.9935, grad_fn=<DivBackward0>)\n",
      "tensor(0.2549)\n",
      "tensor(1.9665, grad_fn=<DivBackward0>)\n",
      "tensor(0.2783)\n",
      "tensor(1.9919, grad_fn=<DivBackward0>)\n",
      "tensor(0.2578)\n",
      "tensor(1.9673, grad_fn=<DivBackward0>)\n",
      "tensor(0.2783)\n",
      "tensor(2.0260, grad_fn=<DivBackward0>)\n",
      "tensor(0.2598)\n",
      "tensor(1.9923, grad_fn=<DivBackward0>)\n",
      "tensor(0.2734)\n",
      "tensor(1.9688, grad_fn=<DivBackward0>)\n",
      "tensor(0.2803)\n",
      "tensor(1.9750, grad_fn=<DivBackward0>)\n",
      "tensor(0.2832)\n",
      "tensor(1.9945, grad_fn=<DivBackward0>)\n",
      "tensor(0.2715)\n",
      "tensor(1.9872, grad_fn=<DivBackward0>)\n",
      "tensor(0.2812)\n",
      "tensor(2.0227, grad_fn=<DivBackward0>)\n",
      "tensor(0.2568)\n",
      "tensor(1.9966, grad_fn=<DivBackward0>)\n",
      "tensor(0.2686)\n",
      "tensor(1.9966, grad_fn=<DivBackward0>)\n",
      "tensor(0.2422)\n",
      "tensor(1.9783, grad_fn=<DivBackward0>)\n",
      "tensor(0.2539)\n",
      "tensor(1.9884, grad_fn=<DivBackward0>)\n",
      "tensor(0.2588)\n",
      "tensor(1.9371, grad_fn=<DivBackward0>)\n",
      "tensor(0.2676)\n",
      "tensor(1.9173, grad_fn=<DivBackward0>)\n",
      "tensor(0.3105)\n",
      "tensor(1.9849, grad_fn=<DivBackward0>)\n",
      "tensor(0.2627)\n",
      "tensor(1.9085, grad_fn=<DivBackward0>)\n",
      "tensor(0.2949)\n",
      "tensor(1.9787, grad_fn=<DivBackward0>)\n",
      "tensor(0.2627)\n",
      "tensor(1.9275, grad_fn=<DivBackward0>)\n",
      "tensor(0.2979)\n",
      "tensor(1.9676, grad_fn=<DivBackward0>)\n",
      "tensor(0.2822)\n",
      "tensor(1.9544, grad_fn=<DivBackward0>)\n",
      "tensor(0.2568)\n",
      "tensor(1.9489, grad_fn=<DivBackward0>)\n",
      "tensor(0.2959)\n",
      "tensor(1.9999, grad_fn=<DivBackward0>)\n",
      "tensor(0.2480)\n",
      "tensor(1.9291, grad_fn=<DivBackward0>)\n",
      "tensor(0.2891)\n",
      "tensor(1.9182, grad_fn=<DivBackward0>)\n",
      "tensor(0.2891)\n",
      "tensor(1.9770, grad_fn=<DivBackward0>)\n",
      "tensor(0.2920)\n",
      "tensor(1.9689, grad_fn=<DivBackward0>)\n",
      "tensor(0.2871)\n",
      "tensor(1.9336, grad_fn=<DivBackward0>)\n",
      "tensor(0.2871)\n",
      "tensor(1.9285, grad_fn=<DivBackward0>)\n",
      "tensor(0.2832)\n",
      "tensor(1.9702, grad_fn=<DivBackward0>)\n",
      "tensor(0.2822)\n",
      "tensor(1.9451, grad_fn=<DivBackward0>)\n",
      "tensor(0.3008)\n",
      "tensor(1.9493, grad_fn=<DivBackward0>)\n",
      "tensor(0.2962)\n",
      "tensor(1.9330, grad_fn=<DivBackward0>)\n",
      "tensor(0.2725)\n",
      "tensor(1.9292, grad_fn=<DivBackward0>)\n",
      "tensor(0.2725)\n",
      "tensor(1.9851, grad_fn=<DivBackward0>)\n",
      "tensor(0.2812)\n",
      "tensor(1.9089, grad_fn=<DivBackward0>)\n",
      "tensor(0.2812)\n",
      "tensor(1.9170, grad_fn=<DivBackward0>)\n",
      "tensor(0.2754)\n",
      "tensor(1.9441, grad_fn=<DivBackward0>)\n",
      "tensor(0.2676)\n",
      "tensor(1.9395, grad_fn=<DivBackward0>)\n",
      "tensor(0.2852)\n",
      "tensor(1.9242, grad_fn=<DivBackward0>)\n",
      "tensor(0.2803)\n",
      "tensor(1.9503, grad_fn=<DivBackward0>)\n",
      "tensor(0.2588)\n",
      "tensor(1.8997, grad_fn=<DivBackward0>)\n",
      "tensor(0.2979)\n",
      "tensor(1.9199, grad_fn=<DivBackward0>)\n",
      "tensor(0.2822)\n",
      "tensor(1.9339, grad_fn=<DivBackward0>)\n",
      "tensor(0.2998)\n",
      "tensor(1.8988, grad_fn=<DivBackward0>)\n",
      "tensor(0.3145)\n",
      "tensor(1.9170, grad_fn=<DivBackward0>)\n",
      "tensor(0.3105)\n",
      "tensor(1.9135, grad_fn=<DivBackward0>)\n",
      "tensor(0.2734)\n",
      "tensor(1.9297, grad_fn=<DivBackward0>)\n",
      "tensor(0.2773)\n",
      "tensor(1.9125, grad_fn=<DivBackward0>)\n",
      "tensor(0.3008)\n",
      "tensor(1.8726, grad_fn=<DivBackward0>)\n",
      "tensor(0.3115)\n",
      "tensor(1.9130, grad_fn=<DivBackward0>)\n",
      "tensor(0.2715)\n",
      "tensor(1.8638, grad_fn=<DivBackward0>)\n",
      "tensor(0.3086)\n",
      "tensor(1.9080, grad_fn=<DivBackward0>)\n",
      "tensor(0.2949)\n",
      "tensor(1.9257, grad_fn=<DivBackward0>)\n",
      "tensor(0.2900)\n",
      "tensor(1.8983, grad_fn=<DivBackward0>)\n",
      "tensor(0.3105)\n",
      "tensor(1.8673, grad_fn=<DivBackward0>)\n",
      "tensor(0.2939)\n",
      "tensor(1.8832, grad_fn=<DivBackward0>)\n",
      "tensor(0.3076)\n",
      "tensor(1.9054, grad_fn=<DivBackward0>)\n",
      "tensor(0.2842)\n",
      "tensor(1.8916, grad_fn=<DivBackward0>)\n",
      "tensor(0.2969)\n",
      "tensor(1.9043, grad_fn=<DivBackward0>)\n",
      "tensor(0.2842)\n",
      "tensor(1.8760, grad_fn=<DivBackward0>)\n",
      "tensor(0.3291)\n",
      "tensor(1.9050, grad_fn=<DivBackward0>)\n",
      "tensor(0.2822)\n",
      "tensor(1.8656, grad_fn=<DivBackward0>)\n",
      "tensor(0.3027)\n",
      "tensor(1.8795, grad_fn=<DivBackward0>)\n",
      "tensor(0.2920)\n",
      "tensor(1.8773, grad_fn=<DivBackward0>)\n",
      "tensor(0.2939)\n",
      "tensor(1.9063, grad_fn=<DivBackward0>)\n",
      "tensor(0.2930)\n",
      "tensor(1.8500, grad_fn=<DivBackward0>)\n",
      "tensor(0.3096)\n",
      "tensor(1.8815, grad_fn=<DivBackward0>)\n",
      "tensor(0.2920)\n",
      "tensor(1.8563, grad_fn=<DivBackward0>)\n",
      "tensor(0.2979)\n",
      "tensor(1.8700, grad_fn=<DivBackward0>)\n",
      "tensor(0.2979)\n",
      "tensor(1.8468, grad_fn=<DivBackward0>)\n",
      "tensor(0.3125)\n",
      "tensor(1.9072, grad_fn=<DivBackward0>)\n",
      "tensor(0.2881)\n",
      "tensor(1.8714, grad_fn=<DivBackward0>)\n",
      "tensor(0.3018)\n",
      "tensor(1.8473, grad_fn=<DivBackward0>)\n",
      "tensor(0.3135)\n",
      "tensor(1.8598, grad_fn=<DivBackward0>)\n",
      "tensor(0.3174)\n",
      "tensor(1.8721, grad_fn=<DivBackward0>)\n",
      "tensor(0.2969)\n",
      "tensor(1.8793, grad_fn=<DivBackward0>)\n",
      "tensor(0.3125)\n",
      "tensor(1.9013, grad_fn=<DivBackward0>)\n",
      "tensor(0.2812)\n",
      "tensor(1.8713, grad_fn=<DivBackward0>)\n",
      "tensor(0.3076)\n",
      "tensor(1.8826, grad_fn=<DivBackward0>)\n",
      "tensor(0.2783)\n",
      "tensor(1.8630, grad_fn=<DivBackward0>)\n",
      "tensor(0.2920)\n",
      "tensor(1.8725, grad_fn=<DivBackward0>)\n",
      "tensor(0.2979)\n",
      "tensor(1.8296, grad_fn=<DivBackward0>)\n",
      "tensor(0.3174)\n",
      "tensor(1.8059, grad_fn=<DivBackward0>)\n",
      "tensor(0.3564)\n",
      "tensor(1.8665, grad_fn=<DivBackward0>)\n",
      "tensor(0.2998)\n",
      "tensor(1.7922, grad_fn=<DivBackward0>)\n",
      "tensor(0.3301)\n",
      "tensor(1.8614, grad_fn=<DivBackward0>)\n",
      "tensor(0.2949)\n",
      "tensor(1.8103, grad_fn=<DivBackward0>)\n",
      "tensor(0.3320)\n",
      "tensor(1.8533, grad_fn=<DivBackward0>)\n",
      "tensor(0.3047)\n",
      "tensor(1.8438, grad_fn=<DivBackward0>)\n",
      "tensor(0.2949)\n",
      "tensor(1.8290, grad_fn=<DivBackward0>)\n",
      "tensor(0.3320)\n",
      "tensor(1.8870, grad_fn=<DivBackward0>)\n",
      "tensor(0.2695)\n",
      "tensor(1.8072, grad_fn=<DivBackward0>)\n",
      "tensor(0.3203)\n",
      "tensor(1.8030, grad_fn=<DivBackward0>)\n",
      "tensor(0.3223)\n",
      "tensor(1.8628, grad_fn=<DivBackward0>)\n",
      "tensor(0.3223)\n",
      "tensor(1.8361, grad_fn=<DivBackward0>)\n",
      "tensor(0.3135)\n",
      "tensor(1.8221, grad_fn=<DivBackward0>)\n",
      "tensor(0.3174)\n",
      "tensor(1.8096, grad_fn=<DivBackward0>)\n",
      "tensor(0.3145)\n",
      "tensor(1.8556, grad_fn=<DivBackward0>)\n",
      "tensor(0.3154)\n",
      "tensor(1.8297, grad_fn=<DivBackward0>)\n",
      "tensor(0.3359)\n",
      "tensor(1.8277, grad_fn=<DivBackward0>)\n",
      "tensor(0.3397)\n",
      "tensor(1.8165, grad_fn=<DivBackward0>)\n",
      "tensor(0.3174)\n",
      "tensor(1.8254, grad_fn=<DivBackward0>)\n",
      "tensor(0.3096)\n",
      "tensor(1.8679, grad_fn=<DivBackward0>)\n",
      "tensor(0.3008)\n",
      "tensor(1.7980, grad_fn=<DivBackward0>)\n",
      "tensor(0.3125)\n",
      "tensor(1.8101, grad_fn=<DivBackward0>)\n",
      "tensor(0.3047)\n",
      "tensor(1.8261, grad_fn=<DivBackward0>)\n",
      "tensor(0.2959)\n",
      "tensor(1.8216, grad_fn=<DivBackward0>)\n",
      "tensor(0.3213)\n",
      "tensor(1.8101, grad_fn=<DivBackward0>)\n",
      "tensor(0.3027)\n",
      "tensor(1.8317, grad_fn=<DivBackward0>)\n",
      "tensor(0.2832)\n",
      "tensor(1.7865, grad_fn=<DivBackward0>)\n",
      "tensor(0.3418)\n",
      "tensor(1.8029, grad_fn=<DivBackward0>)\n",
      "tensor(0.3232)\n",
      "tensor(1.8160, grad_fn=<DivBackward0>)\n",
      "tensor(0.3369)\n",
      "tensor(1.7835, grad_fn=<DivBackward0>)\n",
      "tensor(0.3408)\n",
      "tensor(1.7966, grad_fn=<DivBackward0>)\n",
      "tensor(0.3496)\n",
      "tensor(1.7994, grad_fn=<DivBackward0>)\n",
      "tensor(0.2988)\n",
      "tensor(1.8173, grad_fn=<DivBackward0>)\n",
      "tensor(0.3145)\n",
      "tensor(1.8030, grad_fn=<DivBackward0>)\n",
      "tensor(0.3301)\n",
      "tensor(1.7594, grad_fn=<DivBackward0>)\n",
      "tensor(0.3340)\n",
      "tensor(1.8027, grad_fn=<DivBackward0>)\n",
      "tensor(0.2998)\n",
      "tensor(1.7471, grad_fn=<DivBackward0>)\n",
      "tensor(0.3320)\n",
      "tensor(1.8034, grad_fn=<DivBackward0>)\n",
      "tensor(0.3203)\n",
      "tensor(1.8152, grad_fn=<DivBackward0>)\n",
      "tensor(0.3242)\n",
      "tensor(1.7880, grad_fn=<DivBackward0>)\n",
      "tensor(0.3369)\n",
      "tensor(1.7560, grad_fn=<DivBackward0>)\n",
      "tensor(0.3232)\n",
      "tensor(1.7718, grad_fn=<DivBackward0>)\n",
      "tensor(0.3418)\n",
      "tensor(1.7930, grad_fn=<DivBackward0>)\n",
      "tensor(0.3096)\n",
      "tensor(1.7774, grad_fn=<DivBackward0>)\n",
      "tensor(0.3262)\n",
      "tensor(1.7954, grad_fn=<DivBackward0>)\n",
      "tensor(0.3379)\n",
      "tensor(1.7636, grad_fn=<DivBackward0>)\n",
      "tensor(0.3623)\n",
      "tensor(1.7983, grad_fn=<DivBackward0>)\n",
      "tensor(0.3154)\n",
      "tensor(1.7545, grad_fn=<DivBackward0>)\n",
      "tensor(0.3389)\n",
      "tensor(1.7796, grad_fn=<DivBackward0>)\n",
      "tensor(0.3281)\n",
      "tensor(1.7723, grad_fn=<DivBackward0>)\n",
      "tensor(0.3320)\n",
      "tensor(1.8032, grad_fn=<DivBackward0>)\n",
      "tensor(0.3125)\n",
      "tensor(1.7412, grad_fn=<DivBackward0>)\n",
      "tensor(0.3496)\n",
      "tensor(1.7786, grad_fn=<DivBackward0>)\n",
      "tensor(0.3105)\n",
      "tensor(1.7540, grad_fn=<DivBackward0>)\n",
      "tensor(0.3320)\n",
      "tensor(1.7610, grad_fn=<DivBackward0>)\n",
      "tensor(0.3340)\n",
      "tensor(1.7412, grad_fn=<DivBackward0>)\n",
      "tensor(0.3506)\n",
      "tensor(1.8004, grad_fn=<DivBackward0>)\n",
      "tensor(0.3262)\n",
      "tensor(1.7685, grad_fn=<DivBackward0>)\n",
      "tensor(0.3379)\n",
      "tensor(1.7409, grad_fn=<DivBackward0>)\n",
      "tensor(0.3477)\n",
      "tensor(1.7563, grad_fn=<DivBackward0>)\n",
      "tensor(0.3438)\n",
      "tensor(1.7674, grad_fn=<DivBackward0>)\n",
      "tensor(0.3105)\n",
      "tensor(1.7793, grad_fn=<DivBackward0>)\n",
      "tensor(0.3379)\n",
      "tensor(1.7978, grad_fn=<DivBackward0>)\n",
      "tensor(0.3291)\n",
      "tensor(1.7639, grad_fn=<DivBackward0>)\n",
      "tensor(0.3477)\n",
      "tensor(1.7795, grad_fn=<DivBackward0>)\n",
      "tensor(0.3057)\n",
      "tensor(1.7639, grad_fn=<DivBackward0>)\n",
      "tensor(0.3262)\n",
      "tensor(1.7625, grad_fn=<DivBackward0>)\n",
      "tensor(0.3203)\n",
      "tensor(1.7292, grad_fn=<DivBackward0>)\n",
      "tensor(0.3301)\n",
      "tensor(1.7070, grad_fn=<DivBackward0>)\n",
      "tensor(0.3857)\n",
      "tensor(1.7683, grad_fn=<DivBackward0>)\n",
      "tensor(0.3271)\n",
      "tensor(1.6899, grad_fn=<DivBackward0>)\n",
      "tensor(0.3828)\n",
      "tensor(1.7585, grad_fn=<DivBackward0>)\n",
      "tensor(0.3389)\n",
      "tensor(1.7094, grad_fn=<DivBackward0>)\n",
      "tensor(0.3584)\n",
      "tensor(1.7547, grad_fn=<DivBackward0>)\n",
      "tensor(0.3252)\n",
      "tensor(1.7465, grad_fn=<DivBackward0>)\n",
      "tensor(0.3359)\n",
      "tensor(1.7306, grad_fn=<DivBackward0>)\n",
      "tensor(0.3564)\n",
      "tensor(1.7896, grad_fn=<DivBackward0>)\n",
      "tensor(0.3027)\n",
      "tensor(1.7059, grad_fn=<DivBackward0>)\n",
      "tensor(0.3721)\n",
      "tensor(1.7064, grad_fn=<DivBackward0>)\n",
      "tensor(0.3604)\n",
      "tensor(1.7652, grad_fn=<DivBackward0>)\n",
      "tensor(0.3506)\n",
      "tensor(1.7276, grad_fn=<DivBackward0>)\n",
      "tensor(0.3652)\n",
      "tensor(1.7253, grad_fn=<DivBackward0>)\n",
      "tensor(0.3564)\n",
      "tensor(1.7116, grad_fn=<DivBackward0>)\n",
      "tensor(0.3643)\n",
      "tensor(1.7596, grad_fn=<DivBackward0>)\n",
      "tensor(0.3525)\n",
      "tensor(1.7369, grad_fn=<DivBackward0>)\n",
      "tensor(0.3633)\n",
      "tensor(1.7315, grad_fn=<DivBackward0>)\n",
      "tensor(0.3723)\n",
      "tensor(1.7209, grad_fn=<DivBackward0>)\n",
      "tensor(0.3506)\n",
      "tensor(1.7328, grad_fn=<DivBackward0>)\n",
      "tensor(0.3428)\n",
      "tensor(1.7709, grad_fn=<DivBackward0>)\n",
      "tensor(0.3438)\n",
      "tensor(1.7053, grad_fn=<DivBackward0>)\n",
      "tensor(0.3555)\n",
      "tensor(1.7173, grad_fn=<DivBackward0>)\n",
      "tensor(0.3574)\n",
      "tensor(1.7286, grad_fn=<DivBackward0>)\n",
      "tensor(0.3564)\n",
      "tensor(1.7228, grad_fn=<DivBackward0>)\n",
      "tensor(0.3770)\n",
      "tensor(1.7163, grad_fn=<DivBackward0>)\n",
      "tensor(0.3486)\n",
      "tensor(1.7329, grad_fn=<DivBackward0>)\n",
      "tensor(0.3135)\n",
      "tensor(1.6916, grad_fn=<DivBackward0>)\n",
      "tensor(0.3857)\n",
      "tensor(1.7056, grad_fn=<DivBackward0>)\n",
      "tensor(0.3438)\n",
      "tensor(1.7178, grad_fn=<DivBackward0>)\n",
      "tensor(0.3604)\n",
      "tensor(1.6882, grad_fn=<DivBackward0>)\n",
      "tensor(0.3652)\n",
      "tensor(1.7004, grad_fn=<DivBackward0>)\n",
      "tensor(0.3887)\n",
      "tensor(1.7058, grad_fn=<DivBackward0>)\n",
      "tensor(0.3525)\n",
      "tensor(1.7263, grad_fn=<DivBackward0>)\n",
      "tensor(0.3555)\n",
      "tensor(1.7152, grad_fn=<DivBackward0>)\n",
      "tensor(0.3818)\n",
      "tensor(1.6667, grad_fn=<DivBackward0>)\n",
      "tensor(0.3809)\n",
      "tensor(1.7105, grad_fn=<DivBackward0>)\n",
      "tensor(0.3438)\n",
      "tensor(1.6534, grad_fn=<DivBackward0>)\n",
      "tensor(0.3799)\n",
      "tensor(1.7153, grad_fn=<DivBackward0>)\n",
      "tensor(0.3643)\n",
      "tensor(1.7251, grad_fn=<DivBackward0>)\n",
      "tensor(0.3662)\n",
      "tensor(1.7008, grad_fn=<DivBackward0>)\n",
      "tensor(0.3867)\n",
      "tensor(1.6660, grad_fn=<DivBackward0>)\n",
      "tensor(0.3623)\n",
      "tensor(1.6804, grad_fn=<DivBackward0>)\n",
      "tensor(0.3926)\n",
      "tensor(1.7023, grad_fn=<DivBackward0>)\n",
      "tensor(0.3652)\n",
      "tensor(1.6837, grad_fn=<DivBackward0>)\n",
      "tensor(0.3760)\n",
      "tensor(1.7060, grad_fn=<DivBackward0>)\n",
      "tensor(0.3789)\n",
      "tensor(1.6736, grad_fn=<DivBackward0>)\n",
      "tensor(0.3877)\n",
      "tensor(1.7119, grad_fn=<DivBackward0>)\n",
      "tensor(0.3545)\n",
      "tensor(1.6651, grad_fn=<DivBackward0>)\n",
      "tensor(0.3828)\n",
      "tensor(1.6974, grad_fn=<DivBackward0>)\n",
      "tensor(0.3848)\n",
      "tensor(1.6850, grad_fn=<DivBackward0>)\n",
      "tensor(0.3936)\n",
      "tensor(1.7195, grad_fn=<DivBackward0>)\n",
      "tensor(0.3643)\n",
      "tensor(1.6534, grad_fn=<DivBackward0>)\n",
      "tensor(0.3818)\n",
      "tensor(1.6954, grad_fn=<DivBackward0>)\n",
      "tensor(0.3457)\n",
      "tensor(1.6677, grad_fn=<DivBackward0>)\n",
      "tensor(0.3799)\n",
      "tensor(1.6738, grad_fn=<DivBackward0>)\n",
      "tensor(0.3770)\n",
      "tensor(1.6586, grad_fn=<DivBackward0>)\n",
      "tensor(0.3721)\n",
      "tensor(1.7130, grad_fn=<DivBackward0>)\n",
      "tensor(0.3877)\n",
      "tensor(1.6876, grad_fn=<DivBackward0>)\n",
      "tensor(0.3750)\n",
      "tensor(1.6568, grad_fn=<DivBackward0>)\n",
      "tensor(0.4014)\n",
      "tensor(1.6721, grad_fn=<DivBackward0>)\n",
      "tensor(0.3965)\n",
      "tensor(1.6849, grad_fn=<DivBackward0>)\n",
      "tensor(0.3604)\n",
      "tensor(1.6959, grad_fn=<DivBackward0>)\n",
      "tensor(0.3867)\n",
      "tensor(1.7144, grad_fn=<DivBackward0>)\n",
      "tensor(0.3779)\n",
      "tensor(1.6783, grad_fn=<DivBackward0>)\n",
      "tensor(0.3965)\n",
      "tensor(1.6947, grad_fn=<DivBackward0>)\n",
      "tensor(0.3740)\n",
      "tensor(1.6853, grad_fn=<DivBackward0>)\n",
      "tensor(0.3691)\n",
      "tensor(1.6710, grad_fn=<DivBackward0>)\n",
      "tensor(0.3965)\n",
      "tensor(1.6451, grad_fn=<DivBackward0>)\n",
      "tensor(0.3936)\n",
      "tensor(1.6247, grad_fn=<DivBackward0>)\n",
      "tensor(0.4170)\n",
      "tensor(1.6900, grad_fn=<DivBackward0>)\n",
      "tensor(0.3809)\n",
      "tensor(1.6062, grad_fn=<DivBackward0>)\n",
      "tensor(0.4316)\n",
      "tensor(1.6735, grad_fn=<DivBackward0>)\n",
      "tensor(0.3926)\n",
      "tensor(1.6291, grad_fn=<DivBackward0>)\n",
      "tensor(0.4111)\n",
      "tensor(1.6729, grad_fn=<DivBackward0>)\n",
      "tensor(0.3828)\n",
      "tensor(1.6674, grad_fn=<DivBackward0>)\n",
      "tensor(0.3916)\n",
      "tensor(1.6535, grad_fn=<DivBackward0>)\n",
      "tensor(0.3965)\n",
      "tensor(1.7094, grad_fn=<DivBackward0>)\n",
      "tensor(0.3428)\n",
      "tensor(1.6263, grad_fn=<DivBackward0>)\n",
      "tensor(0.4326)\n",
      "tensor(1.6299, grad_fn=<DivBackward0>)\n",
      "tensor(0.3936)\n",
      "tensor(1.6852, grad_fn=<DivBackward0>)\n",
      "tensor(0.3965)\n",
      "tensor(1.6418, grad_fn=<DivBackward0>)\n",
      "tensor(0.4170)\n",
      "tensor(1.6448, grad_fn=<DivBackward0>)\n",
      "tensor(0.4043)\n",
      "tensor(1.6321, grad_fn=<DivBackward0>)\n",
      "tensor(0.3896)\n",
      "tensor(1.6804, grad_fn=<DivBackward0>)\n",
      "tensor(0.3877)\n",
      "tensor(1.6643, grad_fn=<DivBackward0>)\n",
      "tensor(0.3848)\n",
      "tensor(1.6516, grad_fn=<DivBackward0>)\n",
      "tensor(0.3886)\n",
      "tensor(1.6447, grad_fn=<DivBackward0>)\n",
      "tensor(0.3623)\n",
      "tensor(1.6541, grad_fn=<DivBackward0>)\n",
      "tensor(0.3740)\n",
      "tensor(1.6923, grad_fn=<DivBackward0>)\n",
      "tensor(0.3750)\n",
      "tensor(1.6297, grad_fn=<DivBackward0>)\n",
      "tensor(0.3818)\n",
      "tensor(1.6385, grad_fn=<DivBackward0>)\n",
      "tensor(0.4072)\n",
      "tensor(1.6485, grad_fn=<DivBackward0>)\n",
      "tensor(0.3701)\n",
      "tensor(1.6429, grad_fn=<DivBackward0>)\n",
      "tensor(0.3994)\n",
      "tensor(1.6404, grad_fn=<DivBackward0>)\n",
      "tensor(0.3691)\n",
      "tensor(1.6531, grad_fn=<DivBackward0>)\n",
      "tensor(0.3770)\n",
      "tensor(1.6131, grad_fn=<DivBackward0>)\n",
      "tensor(0.4062)\n",
      "tensor(1.6252, grad_fn=<DivBackward0>)\n",
      "tensor(0.3818)\n",
      "tensor(1.6376, grad_fn=<DivBackward0>)\n",
      "tensor(0.3945)\n",
      "tensor(1.6106, grad_fn=<DivBackward0>)\n",
      "tensor(0.4141)\n",
      "tensor(1.6233, grad_fn=<DivBackward0>)\n",
      "tensor(0.4277)\n",
      "tensor(1.6285, grad_fn=<DivBackward0>)\n",
      "tensor(0.4004)\n",
      "tensor(1.6514, grad_fn=<DivBackward0>)\n",
      "tensor(0.3848)\n",
      "tensor(1.6415, grad_fn=<DivBackward0>)\n",
      "tensor(0.3916)\n",
      "tensor(1.5900, grad_fn=<DivBackward0>)\n",
      "tensor(0.4043)\n",
      "tensor(1.6342, grad_fn=<DivBackward0>)\n",
      "tensor(0.3682)\n",
      "tensor(1.5777, grad_fn=<DivBackward0>)\n",
      "tensor(0.3906)\n",
      "tensor(1.6408, grad_fn=<DivBackward0>)\n",
      "tensor(0.3965)\n",
      "tensor(1.6496, grad_fn=<DivBackward0>)\n",
      "tensor(0.4004)\n",
      "tensor(1.6300, grad_fn=<DivBackward0>)\n",
      "tensor(0.3975)\n",
      "tensor(1.5904, grad_fn=<DivBackward0>)\n",
      "tensor(0.3857)\n",
      "tensor(1.6039, grad_fn=<DivBackward0>)\n",
      "tensor(0.4189)\n",
      "tensor(1.6275, grad_fn=<DivBackward0>)\n",
      "tensor(0.4014)\n",
      "tensor(1.6051, grad_fn=<DivBackward0>)\n",
      "tensor(0.4072)\n",
      "tensor(1.6311, grad_fn=<DivBackward0>)\n",
      "tensor(0.3936)\n",
      "tensor(1.5985, grad_fn=<DivBackward0>)\n",
      "tensor(0.4150)\n",
      "tensor(1.6381, grad_fn=<DivBackward0>)\n",
      "tensor(0.3740)\n",
      "tensor(1.5897, grad_fn=<DivBackward0>)\n",
      "tensor(0.4043)\n",
      "tensor(1.6274, grad_fn=<DivBackward0>)\n",
      "tensor(0.4141)\n",
      "tensor(1.6101, grad_fn=<DivBackward0>)\n",
      "tensor(0.4141)\n",
      "tensor(1.6471, grad_fn=<DivBackward0>)\n",
      "tensor(0.3945)\n",
      "tensor(1.5789, grad_fn=<DivBackward0>)\n",
      "tensor(0.4062)\n",
      "tensor(1.6246, grad_fn=<DivBackward0>)\n",
      "tensor(0.3682)\n",
      "tensor(1.5924, grad_fn=<DivBackward0>)\n",
      "tensor(0.4072)\n",
      "tensor(1.5996, grad_fn=<DivBackward0>)\n",
      "tensor(0.4102)\n",
      "tensor(1.5898, grad_fn=<DivBackward0>)\n",
      "tensor(0.3984)\n",
      "tensor(1.6381, grad_fn=<DivBackward0>)\n",
      "tensor(0.4072)\n",
      "tensor(1.6186, grad_fn=<DivBackward0>)\n",
      "tensor(0.4082)\n",
      "tensor(1.5855, grad_fn=<DivBackward0>)\n",
      "tensor(0.4229)\n",
      "tensor(1.5982, grad_fn=<DivBackward0>)\n",
      "tensor(0.4023)\n",
      "tensor(1.6133, grad_fn=<DivBackward0>)\n",
      "tensor(0.3945)\n",
      "tensor(1.6225, grad_fn=<DivBackward0>)\n",
      "tensor(0.4033)\n",
      "tensor(1.6410, grad_fn=<DivBackward0>)\n",
      "tensor(0.4053)\n",
      "tensor(1.6049, grad_fn=<DivBackward0>)\n",
      "tensor(0.4033)\n",
      "tensor(1.6201, grad_fn=<DivBackward0>)\n",
      "tensor(0.3975)\n",
      "tensor(1.6169, grad_fn=<DivBackward0>)\n",
      "tensor(0.3994)\n",
      "tensor(1.5928, grad_fn=<DivBackward0>)\n",
      "tensor(0.4277)\n",
      "tensor(1.5706, grad_fn=<DivBackward0>)\n",
      "tensor(0.4258)\n",
      "tensor(1.5516, grad_fn=<DivBackward0>)\n",
      "tensor(0.4336)\n",
      "tensor(1.6208, grad_fn=<DivBackward0>)\n",
      "tensor(0.4160)\n",
      "tensor(1.5321, grad_fn=<DivBackward0>)\n",
      "tensor(0.4512)\n",
      "tensor(1.5974, grad_fn=<DivBackward0>)\n",
      "tensor(0.4131)\n",
      "tensor(1.5599, grad_fn=<DivBackward0>)\n",
      "tensor(0.4375)\n",
      "tensor(1.5977, grad_fn=<DivBackward0>)\n",
      "tensor(0.4111)\n",
      "tensor(1.5967, grad_fn=<DivBackward0>)\n",
      "tensor(0.4131)\n",
      "tensor(1.5854, grad_fn=<DivBackward0>)\n",
      "tensor(0.4346)\n",
      "tensor(1.6374, grad_fn=<DivBackward0>)\n",
      "tensor(0.3896)\n",
      "tensor(1.5555, grad_fn=<DivBackward0>)\n",
      "tensor(0.4629)\n",
      "tensor(1.5620, grad_fn=<DivBackward0>)\n",
      "tensor(0.4141)\n",
      "tensor(1.6121, grad_fn=<DivBackward0>)\n",
      "tensor(0.4316)\n",
      "tensor(1.5658, grad_fn=<DivBackward0>)\n",
      "tensor(0.4434)\n",
      "tensor(1.5711, grad_fn=<DivBackward0>)\n",
      "tensor(0.4336)\n",
      "tensor(1.5603, grad_fn=<DivBackward0>)\n",
      "tensor(0.4268)\n",
      "tensor(1.6080, grad_fn=<DivBackward0>)\n",
      "tensor(0.4160)\n",
      "tensor(1.5996, grad_fn=<DivBackward0>)\n",
      "tensor(0.4160)\n",
      "tensor(1.5763, grad_fn=<DivBackward0>)\n",
      "tensor(0.4348)\n",
      "tensor(1.5766, grad_fn=<DivBackward0>)\n",
      "tensor(0.4043)\n",
      "tensor(1.5806, grad_fn=<DivBackward0>)\n",
      "tensor(0.4023)\n",
      "tensor(1.6208, grad_fn=<DivBackward0>)\n",
      "tensor(0.4102)\n",
      "tensor(1.5603, grad_fn=<DivBackward0>)\n",
      "tensor(0.4209)\n",
      "tensor(1.5649, grad_fn=<DivBackward0>)\n",
      "tensor(0.4365)\n",
      "tensor(1.5743, grad_fn=<DivBackward0>)\n",
      "tensor(0.4023)\n",
      "tensor(1.5707, grad_fn=<DivBackward0>)\n",
      "tensor(0.4326)\n",
      "tensor(1.5712, grad_fn=<DivBackward0>)\n",
      "tensor(0.4014)\n",
      "tensor(1.5809, grad_fn=<DivBackward0>)\n",
      "tensor(0.4062)\n",
      "tensor(1.5399, grad_fn=<DivBackward0>)\n",
      "tensor(0.4453)\n",
      "tensor(1.5500, grad_fn=<DivBackward0>)\n",
      "tensor(0.4277)\n",
      "tensor(1.5642, grad_fn=<DivBackward0>)\n",
      "tensor(0.4297)\n",
      "tensor(1.5386, grad_fn=<DivBackward0>)\n",
      "tensor(0.4521)\n",
      "tensor(1.5515, grad_fn=<DivBackward0>)\n",
      "tensor(0.4629)\n",
      "tensor(1.5557, grad_fn=<DivBackward0>)\n",
      "tensor(0.4326)\n",
      "tensor(1.5796, grad_fn=<DivBackward0>)\n",
      "tensor(0.4277)\n",
      "tensor(1.5700, grad_fn=<DivBackward0>)\n",
      "tensor(0.4424)\n",
      "tensor(1.5172, grad_fn=<DivBackward0>)\n",
      "tensor(0.4570)\n",
      "tensor(1.5634, grad_fn=<DivBackward0>)\n",
      "tensor(0.4004)\n",
      "tensor(1.5078, grad_fn=<DivBackward0>)\n",
      "tensor(0.4385)\n",
      "tensor(1.5702, grad_fn=<DivBackward0>)\n",
      "tensor(0.4326)\n",
      "tensor(1.5774, grad_fn=<DivBackward0>)\n",
      "tensor(0.4316)\n",
      "tensor(1.5636, grad_fn=<DivBackward0>)\n",
      "tensor(0.4229)\n",
      "tensor(1.5178, grad_fn=<DivBackward0>)\n",
      "tensor(0.4375)\n",
      "tensor(1.5313, grad_fn=<DivBackward0>)\n",
      "tensor(0.4648)\n",
      "tensor(1.5567, grad_fn=<DivBackward0>)\n",
      "tensor(0.4443)\n",
      "tensor(1.5302, grad_fn=<DivBackward0>)\n",
      "tensor(0.4434)\n",
      "tensor(1.5589, grad_fn=<DivBackward0>)\n",
      "tensor(0.4375)\n",
      "tensor(1.5268, grad_fn=<DivBackward0>)\n",
      "tensor(0.4502)\n",
      "tensor(1.5663, grad_fn=<DivBackward0>)\n",
      "tensor(0.4219)\n",
      "tensor(1.5165, grad_fn=<DivBackward0>)\n",
      "tensor(0.4541)\n",
      "tensor(1.5595, grad_fn=<DivBackward0>)\n",
      "tensor(0.4561)\n",
      "tensor(1.5371, grad_fn=<DivBackward0>)\n",
      "tensor(0.4473)\n",
      "tensor(1.5750, grad_fn=<DivBackward0>)\n",
      "tensor(0.4258)\n",
      "tensor(1.5061, grad_fn=<DivBackward0>)\n",
      "tensor(0.4561)\n",
      "tensor(1.5554, grad_fn=<DivBackward0>)\n",
      "tensor(0.4150)\n",
      "tensor(1.5188, grad_fn=<DivBackward0>)\n",
      "tensor(0.4551)\n",
      "tensor(1.5269, grad_fn=<DivBackward0>)\n",
      "tensor(0.4492)\n",
      "tensor(1.5229, grad_fn=<DivBackward0>)\n",
      "tensor(0.4414)\n",
      "tensor(1.5650, grad_fn=<DivBackward0>)\n",
      "tensor(0.4307)\n",
      "tensor(1.5502, grad_fn=<DivBackward0>)\n",
      "tensor(0.4404)\n",
      "tensor(1.5149, grad_fn=<DivBackward0>)\n",
      "tensor(0.4639)\n",
      "tensor(1.5233, grad_fn=<DivBackward0>)\n",
      "tensor(0.4463)\n",
      "tensor(1.5412, grad_fn=<DivBackward0>)\n",
      "tensor(0.4404)\n",
      "tensor(1.5491, grad_fn=<DivBackward0>)\n",
      "tensor(0.4287)\n",
      "tensor(1.5680, grad_fn=<DivBackward0>)\n",
      "tensor(0.4336)\n",
      "tensor(1.5332, grad_fn=<DivBackward0>)\n",
      "tensor(0.4404)\n",
      "tensor(1.5450, grad_fn=<DivBackward0>)\n",
      "tensor(0.4346)\n",
      "tensor(1.5473, grad_fn=<DivBackward0>)\n",
      "tensor(0.4336)\n",
      "tensor(1.5180, grad_fn=<DivBackward0>)\n",
      "tensor(0.4629)\n",
      "tensor(1.4963, grad_fn=<DivBackward0>)\n",
      "tensor(0.4648)\n",
      "tensor(1.4799, grad_fn=<DivBackward0>)\n",
      "tensor(0.4658)\n",
      "tensor(1.5503, grad_fn=<DivBackward0>)\n",
      "tensor(0.4512)\n",
      "tensor(1.4589, grad_fn=<DivBackward0>)\n",
      "tensor(0.4941)\n",
      "tensor(1.5206, grad_fn=<DivBackward0>)\n",
      "tensor(0.4502)\n",
      "tensor(1.4917, grad_fn=<DivBackward0>)\n",
      "tensor(0.4697)\n",
      "tensor(1.5198, grad_fn=<DivBackward0>)\n",
      "tensor(0.4473)\n",
      "tensor(1.5249, grad_fn=<DivBackward0>)\n",
      "tensor(0.4385)\n",
      "tensor(1.5159, grad_fn=<DivBackward0>)\n",
      "tensor(0.4678)\n",
      "tensor(1.5637, grad_fn=<DivBackward0>)\n",
      "tensor(0.4268)\n",
      "tensor(1.4826, grad_fn=<DivBackward0>)\n",
      "tensor(0.4902)\n",
      "tensor(1.4928, grad_fn=<DivBackward0>)\n",
      "tensor(0.4580)\n",
      "tensor(1.5361, grad_fn=<DivBackward0>)\n",
      "tensor(0.4629)\n",
      "tensor(1.4887, grad_fn=<DivBackward0>)\n",
      "tensor(0.4854)\n",
      "tensor(1.4952, grad_fn=<DivBackward0>)\n",
      "tensor(0.4688)\n",
      "tensor(1.4871, grad_fn=<DivBackward0>)\n",
      "tensor(0.4561)\n",
      "tensor(1.5340, grad_fn=<DivBackward0>)\n",
      "tensor(0.4512)\n",
      "tensor(1.5327, grad_fn=<DivBackward0>)\n",
      "tensor(0.4512)\n",
      "tensor(1.4976, grad_fn=<DivBackward0>)\n",
      "tensor(0.4592)\n",
      "tensor(1.5067, grad_fn=<DivBackward0>)\n",
      "tensor(0.4502)\n",
      "tensor(1.5040, grad_fn=<DivBackward0>)\n",
      "tensor(0.4443)\n",
      "tensor(1.5474, grad_fn=<DivBackward0>)\n",
      "tensor(0.4512)\n",
      "tensor(1.4891, grad_fn=<DivBackward0>)\n",
      "tensor(0.4697)\n",
      "tensor(1.4885, grad_fn=<DivBackward0>)\n",
      "tensor(0.4736)\n",
      "tensor(1.4971, grad_fn=<DivBackward0>)\n",
      "tensor(0.4414)\n",
      "tensor(1.4975, grad_fn=<DivBackward0>)\n",
      "tensor(0.4648)\n",
      "tensor(1.5001, grad_fn=<DivBackward0>)\n",
      "tensor(0.4473)\n",
      "tensor(1.5072, grad_fn=<DivBackward0>)\n",
      "tensor(0.4482)\n",
      "tensor(1.4640, grad_fn=<DivBackward0>)\n",
      "tensor(0.5059)\n",
      "tensor(1.4722, grad_fn=<DivBackward0>)\n",
      "tensor(0.4746)\n",
      "tensor(1.4890, grad_fn=<DivBackward0>)\n",
      "tensor(0.4609)\n",
      "tensor(1.4637, grad_fn=<DivBackward0>)\n",
      "tensor(0.4854)\n",
      "tensor(1.4763, grad_fn=<DivBackward0>)\n",
      "tensor(0.5049)\n",
      "tensor(1.4794, grad_fn=<DivBackward0>)\n",
      "tensor(0.4805)\n",
      "tensor(1.5030, grad_fn=<DivBackward0>)\n",
      "tensor(0.4619)\n",
      "tensor(1.4935, grad_fn=<DivBackward0>)\n",
      "tensor(0.4893)\n",
      "tensor(1.4408, grad_fn=<DivBackward0>)\n",
      "tensor(0.4902)\n",
      "tensor(1.4914, grad_fn=<DivBackward0>)\n",
      "tensor(0.4434)\n",
      "tensor(1.4360, grad_fn=<DivBackward0>)\n",
      "tensor(0.4814)\n",
      "tensor(1.4974, grad_fn=<DivBackward0>)\n",
      "tensor(0.4688)\n",
      "tensor(1.5009, grad_fn=<DivBackward0>)\n",
      "tensor(0.4717)\n",
      "tensor(1.4943, grad_fn=<DivBackward0>)\n",
      "tensor(0.4551)\n",
      "tensor(1.4408, grad_fn=<DivBackward0>)\n",
      "tensor(0.4951)\n",
      "tensor(1.4565, grad_fn=<DivBackward0>)\n",
      "tensor(0.5049)\n",
      "tensor(1.4824, grad_fn=<DivBackward0>)\n",
      "tensor(0.4775)\n",
      "tensor(1.4522, grad_fn=<DivBackward0>)\n",
      "tensor(0.4902)\n",
      "tensor(1.4827, grad_fn=<DivBackward0>)\n",
      "tensor(0.4795)\n",
      "tensor(1.4525, grad_fn=<DivBackward0>)\n",
      "tensor(0.4873)\n",
      "tensor(1.4906, grad_fn=<DivBackward0>)\n",
      "tensor(0.4707)\n",
      "tensor(1.4402, grad_fn=<DivBackward0>)\n",
      "tensor(0.5020)\n",
      "tensor(1.4885, grad_fn=<DivBackward0>)\n",
      "tensor(0.4922)\n",
      "tensor(1.4608, grad_fn=<DivBackward0>)\n",
      "tensor(0.4873)\n",
      "tensor(1.4981, grad_fn=<DivBackward0>)\n",
      "tensor(0.4648)\n",
      "tensor(1.4292, grad_fn=<DivBackward0>)\n",
      "tensor(0.4980)\n",
      "tensor(1.4827, grad_fn=<DivBackward0>)\n",
      "tensor(0.4619)\n",
      "tensor(1.4424, grad_fn=<DivBackward0>)\n",
      "tensor(0.4951)\n",
      "tensor(1.4499, grad_fn=<DivBackward0>)\n",
      "tensor(0.4834)\n",
      "tensor(1.4524, grad_fn=<DivBackward0>)\n",
      "tensor(0.4883)\n",
      "tensor(1.4890, grad_fn=<DivBackward0>)\n",
      "tensor(0.4590)\n",
      "tensor(1.4776, grad_fn=<DivBackward0>)\n",
      "tensor(0.4736)\n",
      "tensor(1.4402, grad_fn=<DivBackward0>)\n",
      "tensor(0.5059)\n",
      "tensor(1.4433, grad_fn=<DivBackward0>)\n",
      "tensor(0.4834)\n",
      "tensor(1.4644, grad_fn=<DivBackward0>)\n",
      "tensor(0.4785)\n",
      "tensor(1.4722, grad_fn=<DivBackward0>)\n",
      "tensor(0.4727)\n",
      "tensor(1.4919, grad_fn=<DivBackward0>)\n",
      "tensor(0.4697)\n",
      "tensor(1.4585, grad_fn=<DivBackward0>)\n",
      "tensor(0.4746)\n",
      "tensor(1.4663, grad_fn=<DivBackward0>)\n",
      "tensor(0.4805)\n",
      "tensor(1.4730, grad_fn=<DivBackward0>)\n",
      "tensor(0.4844)\n",
      "tensor(1.4419, grad_fn=<DivBackward0>)\n",
      "tensor(0.4951)\n",
      "tensor(1.4193, grad_fn=<DivBackward0>)\n",
      "tensor(0.5029)\n",
      "tensor(1.4064, grad_fn=<DivBackward0>)\n",
      "tensor(0.4922)\n",
      "tensor(1.4750, grad_fn=<DivBackward0>)\n",
      "tensor(0.4951)\n",
      "tensor(1.3837, grad_fn=<DivBackward0>)\n",
      "tensor(0.5195)\n",
      "tensor(1.4403, grad_fn=<DivBackward0>)\n",
      "tensor(0.4863)\n",
      "tensor(1.4212, grad_fn=<DivBackward0>)\n",
      "tensor(0.5068)\n",
      "tensor(1.4376, grad_fn=<DivBackward0>)\n",
      "tensor(0.4863)\n",
      "tensor(1.4505, grad_fn=<DivBackward0>)\n",
      "tensor(0.4736)\n",
      "tensor(1.4427, grad_fn=<DivBackward0>)\n",
      "tensor(0.4902)\n",
      "tensor(1.4854, grad_fn=<DivBackward0>)\n",
      "tensor(0.4707)\n",
      "tensor(1.4058, grad_fn=<DivBackward0>)\n",
      "tensor(0.5254)\n",
      "tensor(1.4195, grad_fn=<DivBackward0>)\n",
      "tensor(0.4912)\n",
      "tensor(1.4563, grad_fn=<DivBackward0>)\n",
      "tensor(0.5039)\n",
      "tensor(1.4090, grad_fn=<DivBackward0>)\n",
      "tensor(0.5254)\n",
      "tensor(1.4155, grad_fn=<DivBackward0>)\n",
      "tensor(0.5000)\n",
      "tensor(1.4115, grad_fn=<DivBackward0>)\n",
      "tensor(0.4961)\n",
      "tensor(1.4569, grad_fn=<DivBackward0>)\n",
      "tensor(0.4873)\n",
      "tensor(1.4618, grad_fn=<DivBackward0>)\n",
      "tensor(0.4971)\n",
      "tensor(1.4166, grad_fn=<DivBackward0>)\n",
      "tensor(0.4946)\n",
      "tensor(1.4340, grad_fn=<DivBackward0>)\n",
      "tensor(0.4932)\n",
      "tensor(1.4240, grad_fn=<DivBackward0>)\n",
      "tensor(0.4863)\n",
      "tensor(1.4710, grad_fn=<DivBackward0>)\n",
      "tensor(0.4785)\n",
      "tensor(1.4151, grad_fn=<DivBackward0>)\n",
      "tensor(0.5107)\n",
      "tensor(1.4086, grad_fn=<DivBackward0>)\n",
      "tensor(0.5127)\n",
      "tensor(1.4172, grad_fn=<DivBackward0>)\n",
      "tensor(0.4854)\n",
      "tensor(1.4235, grad_fn=<DivBackward0>)\n",
      "tensor(0.5029)\n",
      "tensor(1.4262, grad_fn=<DivBackward0>)\n",
      "tensor(0.4854)\n",
      "tensor(1.4316, grad_fn=<DivBackward0>)\n",
      "tensor(0.4795)\n",
      "tensor(1.3860, grad_fn=<DivBackward0>)\n",
      "tensor(0.5488)\n",
      "tensor(1.3923, grad_fn=<DivBackward0>)\n",
      "tensor(0.5088)\n",
      "tensor(1.4117, grad_fn=<DivBackward0>)\n",
      "tensor(0.4922)\n",
      "tensor(1.3857, grad_fn=<DivBackward0>)\n",
      "tensor(0.5078)\n",
      "tensor(1.3979, grad_fn=<DivBackward0>)\n",
      "tensor(0.5254)\n",
      "tensor(1.4004, grad_fn=<DivBackward0>)\n",
      "tensor(0.5137)\n",
      "tensor(1.4227, grad_fn=<DivBackward0>)\n",
      "tensor(0.5000)\n",
      "tensor(1.4150, grad_fn=<DivBackward0>)\n",
      "tensor(0.5234)\n",
      "tensor(1.3620, grad_fn=<DivBackward0>)\n",
      "tensor(0.5273)\n",
      "tensor(1.4180, grad_fn=<DivBackward0>)\n",
      "tensor(0.4873)\n",
      "tensor(1.3631, grad_fn=<DivBackward0>)\n",
      "tensor(0.5137)\n",
      "tensor(1.4230, grad_fn=<DivBackward0>)\n",
      "tensor(0.4941)\n",
      "tensor(1.4218, grad_fn=<DivBackward0>)\n",
      "tensor(0.5020)\n",
      "tensor(1.4226, grad_fn=<DivBackward0>)\n",
      "tensor(0.5059)\n",
      "tensor(1.3615, grad_fn=<DivBackward0>)\n",
      "tensor(0.5391)\n",
      "tensor(1.3807, grad_fn=<DivBackward0>)\n",
      "tensor(0.5420)\n",
      "tensor(1.4057, grad_fn=<DivBackward0>)\n",
      "tensor(0.5117)\n",
      "tensor(1.3738, grad_fn=<DivBackward0>)\n",
      "tensor(0.5283)\n",
      "tensor(1.4044, grad_fn=<DivBackward0>)\n",
      "tensor(0.5068)\n",
      "tensor(1.3781, grad_fn=<DivBackward0>)\n",
      "tensor(0.5215)\n",
      "tensor(1.4135, grad_fn=<DivBackward0>)\n",
      "tensor(0.5039)\n",
      "tensor(1.3631, grad_fn=<DivBackward0>)\n",
      "tensor(0.5410)\n",
      "tensor(1.4161, grad_fn=<DivBackward0>)\n",
      "tensor(0.5283)\n",
      "tensor(1.3836, grad_fn=<DivBackward0>)\n",
      "tensor(0.5273)\n",
      "tensor(1.4199, grad_fn=<DivBackward0>)\n",
      "tensor(0.5029)\n",
      "tensor(1.3508, grad_fn=<DivBackward0>)\n",
      "tensor(0.5312)\n",
      "tensor(1.4086, grad_fn=<DivBackward0>)\n",
      "tensor(0.4990)\n",
      "tensor(1.3652, grad_fn=<DivBackward0>)\n",
      "tensor(0.5459)\n",
      "tensor(1.3715, grad_fn=<DivBackward0>)\n",
      "tensor(0.5176)\n",
      "tensor(1.3809, grad_fn=<DivBackward0>)\n",
      "tensor(0.5234)\n",
      "tensor(1.4130, grad_fn=<DivBackward0>)\n",
      "tensor(0.4980)\n",
      "tensor(1.4029, grad_fn=<DivBackward0>)\n",
      "tensor(0.4971)\n",
      "tensor(1.3636, grad_fn=<DivBackward0>)\n",
      "tensor(0.5342)\n",
      "tensor(1.3623, grad_fn=<DivBackward0>)\n",
      "tensor(0.5352)\n",
      "tensor(1.3860, grad_fn=<DivBackward0>)\n",
      "tensor(0.5166)\n",
      "tensor(1.3944, grad_fn=<DivBackward0>)\n",
      "tensor(0.5176)\n",
      "tensor(1.4167, grad_fn=<DivBackward0>)\n",
      "tensor(0.5000)\n",
      "tensor(1.3842, grad_fn=<DivBackward0>)\n",
      "tensor(0.5176)\n",
      "tensor(1.3884, grad_fn=<DivBackward0>)\n",
      "tensor(0.4990)\n",
      "tensor(1.3975, grad_fn=<DivBackward0>)\n",
      "tensor(0.5225)\n",
      "tensor(1.3671, grad_fn=<DivBackward0>)\n",
      "tensor(0.5205)\n",
      "tensor(1.3424, grad_fn=<DivBackward0>)\n",
      "tensor(0.5430)\n",
      "tensor(1.3343, grad_fn=<DivBackward0>)\n",
      "tensor(0.5264)\n",
      "tensor(1.3984, grad_fn=<DivBackward0>)\n",
      "tensor(0.5244)\n",
      "tensor(1.3098, grad_fn=<DivBackward0>)\n",
      "tensor(0.5527)\n",
      "tensor(1.3602, grad_fn=<DivBackward0>)\n",
      "tensor(0.5186)\n",
      "tensor(1.3510, grad_fn=<DivBackward0>)\n",
      "tensor(0.5381)\n",
      "tensor(1.3560, grad_fn=<DivBackward0>)\n",
      "tensor(0.5244)\n",
      "tensor(1.3777, grad_fn=<DivBackward0>)\n",
      "tensor(0.5039)\n",
      "tensor(1.3685, grad_fn=<DivBackward0>)\n",
      "tensor(0.5225)\n",
      "tensor(1.4064, grad_fn=<DivBackward0>)\n",
      "tensor(0.5088)\n",
      "tensor(1.3295, grad_fn=<DivBackward0>)\n",
      "tensor(0.5430)\n",
      "tensor(1.3450, grad_fn=<DivBackward0>)\n",
      "tensor(0.5283)\n",
      "tensor(1.3774, grad_fn=<DivBackward0>)\n",
      "tensor(0.5439)\n",
      "tensor(1.3300, grad_fn=<DivBackward0>)\n",
      "tensor(0.5586)\n",
      "tensor(1.3355, grad_fn=<DivBackward0>)\n",
      "tensor(0.5381)\n",
      "tensor(1.3368, grad_fn=<DivBackward0>)\n",
      "tensor(0.5371)\n",
      "tensor(1.3808, grad_fn=<DivBackward0>)\n",
      "tensor(0.5215)\n",
      "tensor(1.3900, grad_fn=<DivBackward0>)\n",
      "tensor(0.5322)\n",
      "tensor(1.3388, grad_fn=<DivBackward0>)\n",
      "tensor(0.5245)\n",
      "tensor(1.3624, grad_fn=<DivBackward0>)\n",
      "tensor(0.5234)\n",
      "tensor(1.3455, grad_fn=<DivBackward0>)\n",
      "tensor(0.5332)\n",
      "tensor(1.3954, grad_fn=<DivBackward0>)\n",
      "tensor(0.5166)\n",
      "tensor(1.3415, grad_fn=<DivBackward0>)\n",
      "tensor(0.5400)\n",
      "tensor(1.3292, grad_fn=<DivBackward0>)\n",
      "tensor(0.5449)\n",
      "tensor(1.3392, grad_fn=<DivBackward0>)\n",
      "tensor(0.5225)\n",
      "tensor(1.3525, grad_fn=<DivBackward0>)\n",
      "tensor(0.5361)\n",
      "tensor(1.3525, grad_fn=<DivBackward0>)\n",
      "tensor(0.5195)\n",
      "tensor(1.3578, grad_fn=<DivBackward0>)\n",
      "tensor(0.5254)\n",
      "tensor(1.3104, grad_fn=<DivBackward0>)\n",
      "tensor(0.5723)\n",
      "tensor(1.3144, grad_fn=<DivBackward0>)\n",
      "tensor(0.5488)\n",
      "tensor(1.3356, grad_fn=<DivBackward0>)\n",
      "tensor(0.5312)\n",
      "tensor(1.3095, grad_fn=<DivBackward0>)\n",
      "tensor(0.5381)\n",
      "tensor(1.3213, grad_fn=<DivBackward0>)\n",
      "tensor(0.5723)\n",
      "tensor(1.3229, grad_fn=<DivBackward0>)\n",
      "tensor(0.5508)\n",
      "tensor(1.3437, grad_fn=<DivBackward0>)\n",
      "tensor(0.5508)\n",
      "tensor(1.3391, grad_fn=<DivBackward0>)\n",
      "tensor(0.5576)\n",
      "tensor(1.2845, grad_fn=<DivBackward0>)\n",
      "tensor(0.5576)\n",
      "tensor(1.3464, grad_fn=<DivBackward0>)\n",
      "tensor(0.5283)\n",
      "tensor(1.2926, grad_fn=<DivBackward0>)\n",
      "tensor(0.5518)\n",
      "tensor(1.3499, grad_fn=<DivBackward0>)\n",
      "tensor(0.5225)\n",
      "tensor(1.3452, grad_fn=<DivBackward0>)\n",
      "tensor(0.5469)\n",
      "tensor(1.3517, grad_fn=<DivBackward0>)\n",
      "tensor(0.5430)\n",
      "tensor(1.2841, grad_fn=<DivBackward0>)\n",
      "tensor(0.5684)\n",
      "tensor(1.3075, grad_fn=<DivBackward0>)\n",
      "tensor(0.5693)\n",
      "tensor(1.3303, grad_fn=<DivBackward0>)\n",
      "tensor(0.5518)\n",
      "tensor(1.2992, grad_fn=<DivBackward0>)\n",
      "tensor(0.5684)\n",
      "tensor(1.3282, grad_fn=<DivBackward0>)\n",
      "tensor(0.5469)\n",
      "tensor(1.3074, grad_fn=<DivBackward0>)\n",
      "tensor(0.5508)\n",
      "tensor(1.3392, grad_fn=<DivBackward0>)\n",
      "tensor(0.5469)\n",
      "tensor(1.2886, grad_fn=<DivBackward0>)\n",
      "tensor(0.5752)\n",
      "tensor(1.3456, grad_fn=<DivBackward0>)\n",
      "tensor(0.5742)\n",
      "tensor(1.3089, grad_fn=<DivBackward0>)\n",
      "tensor(0.5547)\n",
      "tensor(1.3438, grad_fn=<DivBackward0>)\n",
      "tensor(0.5371)\n",
      "tensor(1.2747, grad_fn=<DivBackward0>)\n",
      "tensor(0.5713)\n",
      "tensor(1.3368, grad_fn=<DivBackward0>)\n",
      "tensor(0.5381)\n",
      "tensor(1.2905, grad_fn=<DivBackward0>)\n",
      "tensor(0.5791)\n",
      "tensor(1.2950, grad_fn=<DivBackward0>)\n",
      "tensor(0.5576)\n",
      "tensor(1.3114, grad_fn=<DivBackward0>)\n",
      "tensor(0.5654)\n",
      "tensor(1.3401, grad_fn=<DivBackward0>)\n",
      "tensor(0.5332)\n",
      "tensor(1.3289, grad_fn=<DivBackward0>)\n",
      "tensor(0.5381)\n",
      "tensor(1.2883, grad_fn=<DivBackward0>)\n",
      "tensor(0.5693)\n",
      "tensor(1.2843, grad_fn=<DivBackward0>)\n",
      "tensor(0.5703)\n",
      "tensor(1.3093, grad_fn=<DivBackward0>)\n",
      "tensor(0.5645)\n",
      "tensor(1.3182, grad_fn=<DivBackward0>)\n",
      "tensor(0.5469)\n",
      "tensor(1.3450, grad_fn=<DivBackward0>)\n",
      "tensor(0.5352)\n",
      "tensor(1.3136, grad_fn=<DivBackward0>)\n",
      "tensor(0.5420)\n",
      "tensor(1.3152, grad_fn=<DivBackward0>)\n",
      "tensor(0.5312)\n",
      "tensor(1.3239, grad_fn=<DivBackward0>)\n",
      "tensor(0.5537)\n",
      "tensor(1.2957, grad_fn=<DivBackward0>)\n",
      "tensor(0.5547)\n",
      "tensor(1.2685, grad_fn=<DivBackward0>)\n",
      "tensor(0.5732)\n",
      "tensor(1.2657, grad_fn=<DivBackward0>)\n",
      "tensor(0.5605)\n",
      "tensor(1.3236, grad_fn=<DivBackward0>)\n",
      "tensor(0.5527)\n",
      "tensor(1.2394, grad_fn=<DivBackward0>)\n",
      "tensor(0.5889)\n",
      "tensor(1.2826, grad_fn=<DivBackward0>)\n",
      "tensor(0.5469)\n",
      "tensor(1.2830, grad_fn=<DivBackward0>)\n",
      "tensor(0.5527)\n",
      "tensor(1.2788, grad_fn=<DivBackward0>)\n",
      "tensor(0.5664)\n",
      "tensor(1.3084, grad_fn=<DivBackward0>)\n",
      "tensor(0.5410)\n",
      "tensor(1.2952, grad_fn=<DivBackward0>)\n",
      "tensor(0.5674)\n",
      "tensor(1.3294, grad_fn=<DivBackward0>)\n",
      "tensor(0.5391)\n",
      "tensor(1.2566, grad_fn=<DivBackward0>)\n",
      "tensor(0.5830)\n",
      "tensor(1.2722, grad_fn=<DivBackward0>)\n",
      "tensor(0.5654)\n",
      "tensor(1.3030, grad_fn=<DivBackward0>)\n",
      "tensor(0.5752)\n",
      "tensor(1.2540, grad_fn=<DivBackward0>)\n",
      "tensor(0.5889)\n",
      "tensor(1.2576, grad_fn=<DivBackward0>)\n",
      "tensor(0.5684)\n",
      "tensor(1.2653, grad_fn=<DivBackward0>)\n",
      "tensor(0.5713)\n",
      "tensor(1.3080, grad_fn=<DivBackward0>)\n",
      "tensor(0.5488)\n",
      "tensor(1.3190, grad_fn=<DivBackward0>)\n",
      "tensor(0.5645)\n",
      "tensor(1.2663, grad_fn=<DivBackward0>)\n",
      "tensor(0.5652)\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    for j in range(0, len(X), batch_size):\n",
    "        \n",
    "        X_batch = X_t[j: j + batch_size]\n",
    "        y_batch = y_t[j: j + batch_size]\n",
    "\n",
    "\n",
    "        w1.requires_grad = True\n",
    "        b1.requires_grad = True\n",
    "        w2.requires_grad = True\n",
    "        b2.requires_grad = True\n",
    "\n",
    "\n",
    "        z1 = X_batch @ w1 + b1\n",
    "    #     print(torch.norm(z1))\n",
    "        a1 = torch.max(torch.zeros_like(z1), z1)\n",
    "    #     a1 = torch.nn.functional.relu(z1)\n",
    "    #     print(torch.norm(a1))\n",
    "\n",
    "        z2 = a1 @ w2 + b2\n",
    "        sm_scale = z2 - torch.max(z2, dim=-1, keepdims=True)[0]\n",
    "        y__t = torch.exp(sm_scale) / torch.sum(torch.exp(sm_scale), keepdims=True, axis=-1)\n",
    "        assert y__t.shape == y_batch.shape\n",
    "    #     print(torch.norm(y__t))\n",
    "        logy = torch.log(y__t)\n",
    "    #     logy.requires_grad = True\n",
    "\n",
    "    #     print(torch.norm(logy))\n",
    "        ce = - y_batch * logy\n",
    "        l = torch.sum(ce) / len(y_batch)\n",
    "    #     l = torch.sum(ce)\n",
    "        l.backward()\n",
    "#         print(torch.norm(w1.grad))\n",
    "#         print(torch.norm(w2.grad))\n",
    "\n",
    "        if j % 8 * batch_size == 0:\n",
    "            print(l)\n",
    "            print((torch.max(y__t, dim=-1)[1] == torch.max(y_batch, dim=-1)[1]).sum().float() / len(y_batch))\n",
    "\n",
    "        w1 = w1.data - lr * w1.grad\n",
    "        w2 = w2.data - lr * w2.grad\n",
    "\n",
    "        b1 = b1.data - lr * b1.grad\n",
    "        b2 = b2.data - lr * b2.grad\n",
    "\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear:\n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        self.w = np.zeros([in_dim, out_dim])\n",
    "        self.w = np.repeat(np.linspace(-0.1, 0.1, out_dim)[np.newaxis, ...], in_dim, axis=0)\n",
    "\n",
    "#         self.w = np.random.randn(in_dim, out_dim)\n",
    "        self.b = np.zeros([1, out_dim])\n",
    "        self.dw = None\n",
    "        self.db = None\n",
    "        self.in_dim = self.w.shape[0]\n",
    "        self.out_dim = self.w.shape[1]\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "        return np.matmul(x, self.w) + self.b\n",
    "    \n",
    "    def backward(self, d):\n",
    "        self.db = np.mean(d, axis=0)\n",
    "        assert self.db.shape == self.b.shape, (d.shape, self.db.shape, self.b.shape)\n",
    "        \n",
    "        J = np.zeros([self.x.shape[0], self.out_dim, np.prod(self.w.shape)])\n",
    "        j = 0\n",
    "        for i in range(self.out_dim):\n",
    "            J[:, i: i + 1, j: j + self.in_dim] = self.x[:, np.newaxis, :]\n",
    "            j += self.in_dim\n",
    "        \n",
    "        dw = d @ J\n",
    "        \n",
    "        dw = np.reshape(np.mean(dw, axis=0), self.w.shape, order='F')\n",
    "        \n",
    "        self.dw = dw\n",
    "        \n",
    "        d = d @ np.repeat(self.w.T[np.newaxis, ...], d.shape[0], axis=0)\n",
    "        \n",
    "        return d\n",
    "        \n",
    "    def step(self, lr):\n",
    "        self.w = self.w - lr * self.dw\n",
    "        self.b = self.b - lr * self.db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU:\n",
    "    def __init__(self):\n",
    "        self.a = None\n",
    "        \n",
    "    def forward(self, x):\n",
    "        self.a = np.maximum(x, 0)\n",
    "        return self.a\n",
    "    \n",
    "    def backward(self, d):\n",
    "        return d * (self.a != 0)[:, np.newaxis, :].astype(np.float32)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Softmax:\n",
    "    def __init__(self):\n",
    "        self.a = None\n",
    "        \n",
    "    def forward(self, x):\n",
    "        assert len(x.shape) == 2\n",
    "        x = x - np.max(x, axis=-1, keepdims=True)\n",
    "        self.a = np.exp(x) / np.sum(np.exp(x), keepdims=True, axis=-1)\n",
    "        return self.a\n",
    "    def backward(self, d):\n",
    "        \n",
    "        diag = np.stack([np.diag(self.a[i]) for i in range(len(self.a))])\n",
    "        op = np.stack([np.outer(self.a[i], self.a[i]) for i in range(len(self.a))])\n",
    "        J = diag - op\n",
    "        \n",
    "        return d[:, np.newaxis, ...] @ J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossEntropy:\n",
    "    def forward(self, y_, y):\n",
    "        \n",
    "        l = - np.sum(y * np.log(y_))\n",
    "        l /= len(y)\n",
    "        return y_, l\n",
    "    \n",
    "    def backward(self, y_, y):\n",
    "        assert y_.shape == y.shape\n",
    "        d =  - y / y_\n",
    "        return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP:\n",
    "    def __init__(self):\n",
    "        self.linear1 = Linear(784, 32)\n",
    "        self.relu1 = ReLU()\n",
    "        self.linear2 = Linear(32, 10)\n",
    "#         self.relu2 = ReLU()\n",
    "#         self.linear3 = Linear(32, 10)\n",
    "        self.softmax = Softmax()\n",
    "        self.loss = CrossEntropy()\n",
    "        \n",
    "    def forward(self, x, y):\n",
    "        x = self.linear1.forward(x)\n",
    "        x = self.relu1.forward(x)\n",
    "        x = self.linear2.forward(x)\n",
    "#         x = self.relu2.forward(x)\n",
    "#         x = self.linear3.forward(x)\n",
    "        x = self.softmax.forward(x)\n",
    "        loss = self.loss.forward(x, y)\n",
    "        return loss\n",
    "    \n",
    "    def backward(self, y_, y):\n",
    "        d = self.loss.backward(y_, y)\n",
    "        d = self.softmax.backward(d)\n",
    "#         d = self.linear3.backward(d)\n",
    "#         d = self.relu2.backward(d)\n",
    "        d = self.linear2.backward(d)\n",
    "        d = self.relu1.backward(d)\n",
    "        d = self.linear1.backward(d)\n",
    "\n",
    "    \n",
    "    def step(self, lr):\n",
    "#         print(np.linalg.norm(self.linear1.dw))\n",
    "        self.linear1.step(lr)\n",
    "#         print(np.linalg.norm(self.linear2.dw))\n",
    "        self.linear2.step(lr)\n",
    "#         self.linear3.step(lr)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLP()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.050843424115447 loss\n",
      "0.2275390625\n",
      "2.0244736062109494 loss\n",
      "0.2373046875\n",
      "2.1053338333531864 loss\n",
      "0.228515625\n",
      "2.02092516573021 loss\n",
      "0.2470703125\n",
      "2.0224373295976354 loss\n",
      "0.2451171875\n",
      "2.0677909596961075 loss\n",
      "0.2236328125\n",
      "2.058034708956967 loss\n",
      "0.2412109375\n",
      "2.040425398074608 loss\n",
      "0.2353515625\n",
      "2.0685329166410606 loss\n",
      "0.2255859375\n",
      "2.0138914015778093 loss\n",
      "0.2548828125\n",
      "2.0402885936706388 loss\n",
      "0.2353515625\n",
      "2.053788074151969 loss\n",
      "0.236328125\n",
      "2.014137313971744 loss\n",
      "0.2578125\n",
      "2.044875742114674 loss\n",
      "0.2607421875\n",
      "2.0332065063579243 loss\n",
      "0.23828125\n",
      "2.0482528123948907 loss\n",
      "0.244140625\n",
      "2.0318649332337495 loss\n",
      "0.2666015625\n",
      "1.9916577757922862 loss\n",
      "0.267578125\n",
      "2.0268667820872572 loss\n",
      "0.240234375\n",
      "1.9878172008691142 loss\n",
      "0.267578125\n",
      "2.0171780746230494 loss\n",
      "0.2607421875\n",
      "2.0438002106453106 loss\n",
      "0.251953125\n",
      "2.0181849706575283 loss\n",
      "0.2744140625\n",
      "1.9880703013141579 loss\n",
      "0.251953125\n",
      "2.0029565742292172 loss\n",
      "0.2685546875\n",
      "2.025949639183146 loss\n",
      "0.2451171875\n",
      "2.0143684647519624 loss\n",
      "0.2578125\n",
      "2.0188957316625613 loss\n",
      "0.2529296875\n",
      "1.9986795148483711 loss\n",
      "0.28125\n",
      "2.0225767014733136 loss\n",
      "0.2431640625\n",
      "1.9868682216311426 loss\n",
      "0.2509765625\n",
      "1.9865867384824187 loss\n",
      "0.2744140625\n",
      "1.9900403771722157 loss\n",
      "0.2568359375\n",
      "2.0200628390824003 loss\n",
      "0.265625\n",
      "1.9692540877367577 loss\n",
      "0.2763671875\n",
      "1.9934638483823306 loss\n",
      "0.2548828125\n",
      "1.9664632612662345 loss\n",
      "0.2783203125\n",
      "1.9918642004528526 loss\n",
      "0.2578125\n",
      "1.9673351541749446 loss\n",
      "0.2783203125\n",
      "2.0260298303376842 loss\n",
      "0.259765625\n",
      "1.992348628988894 loss\n",
      "0.2734375\n",
      "1.9688492239739994 loss\n",
      "0.2802734375\n",
      "1.9749652713671266 loss\n",
      "0.283203125\n",
      "1.9945067377700751 loss\n",
      "0.271484375\n",
      "1.9871863051996868 loss\n",
      "0.28125\n",
      "2.022722088428133 loss\n",
      "0.2568359375\n",
      "1.9965858306944835 loss\n",
      "0.2685546875\n",
      "1.996580227954234 loss\n",
      "0.2421875\n",
      "1.9782818403747364 loss\n",
      "0.25390625\n",
      "1.9883750349015217 loss\n",
      "0.2587890625\n",
      "1.9370585895929793 loss\n",
      "0.267578125\n",
      "1.9173471836684177 loss\n",
      "0.310546875\n",
      "1.984851470707706 loss\n",
      "0.2626953125\n",
      "1.9085163277271526 loss\n",
      "0.294921875\n",
      "1.9786760215079078 loss\n",
      "0.2626953125\n",
      "1.9275128473090306 loss\n",
      "0.2978515625\n",
      "1.9675583487396633 loss\n",
      "0.2822265625\n",
      "1.9543650030059285 loss\n",
      "0.2568359375\n",
      "1.9489187890158337 loss\n",
      "0.2958984375\n",
      "1.9998755310935135 loss\n",
      "0.248046875\n",
      "1.929076072315735 loss\n",
      "0.2890625\n",
      "1.918155301247728 loss\n",
      "0.2890625\n",
      "1.9769508972158776 loss\n",
      "0.2919921875\n",
      "1.9689044497865171 loss\n",
      "0.287109375\n",
      "1.9336200029455282 loss\n",
      "0.287109375\n",
      "1.9284694069580914 loss\n",
      "0.283203125\n",
      "1.9701905628872984 loss\n",
      "0.2822265625\n",
      "1.9451287462438518 loss\n",
      "0.30078125\n",
      "1.949305437811827 loss\n",
      "0.296195652173913\n",
      "1.9330029804343072 loss\n",
      "0.2724609375\n",
      "1.9291876178053666 loss\n",
      "0.2724609375\n",
      "1.9851308130742376 loss\n",
      "0.28125\n",
      "1.9088839649942042 loss\n",
      "0.28125\n",
      "1.9169514346659926 loss\n",
      "0.275390625\n",
      "1.9441419142630512 loss\n",
      "0.267578125\n",
      "1.9394768518096392 loss\n",
      "0.28515625\n",
      "1.9241889886134822 loss\n",
      "0.2802734375\n",
      "1.9502830871839525 loss\n",
      "0.2587890625\n",
      "1.8997124838605859 loss\n",
      "0.2978515625\n",
      "1.9199472641379325 loss\n",
      "0.2822265625\n",
      "1.9339330281477678 loss\n",
      "0.2998046875\n",
      "1.8988246091317929 loss\n",
      "0.314453125\n",
      "1.916976046225909 loss\n",
      "0.310546875\n",
      "1.9135148756301363 loss\n",
      "0.2734375\n",
      "1.9296685212256297 loss\n",
      "0.27734375\n",
      "1.9125024675417577 loss\n",
      "0.30078125\n",
      "1.8726408944966688 loss\n",
      "0.3115234375\n",
      "1.9129649270555942 loss\n",
      "0.271484375\n",
      "1.8638138486660245 loss\n",
      "0.30859375\n",
      "1.907990266329093 loss\n",
      "0.294921875\n",
      "1.9257008036135628 loss\n",
      "0.2900390625\n",
      "1.8983439895675425 loss\n",
      "0.310546875\n",
      "1.867328631754406 loss\n",
      "0.2939453125\n",
      "1.8832041536518425 loss\n",
      "0.3076171875\n",
      "1.905379059579 loss\n",
      "0.2841796875\n",
      "1.8916190455717077 loss\n",
      "0.296875\n",
      "1.9043011206828098 loss\n",
      "0.2841796875\n",
      "1.8759618774135542 loss\n",
      "0.3291015625\n",
      "1.9049646189062743 loss\n",
      "0.2822265625\n",
      "1.8655963029615168 loss\n",
      "0.302734375\n",
      "1.8795217476156565 loss\n",
      "0.2919921875\n",
      "1.8772820293768517 loss\n",
      "0.2939453125\n",
      "1.9062898271045041 loss\n",
      "0.29296875\n",
      "1.8499617283067644 loss\n",
      "0.3095703125\n",
      "1.8814783234567607 loss\n",
      "0.2919921875\n",
      "1.8563026786568426 loss\n",
      "0.2978515625\n",
      "1.870028629543008 loss\n",
      "0.2978515625\n",
      "1.8468095417183719 loss\n",
      "0.3125\n",
      "1.9072146395633731 loss\n",
      "0.2880859375\n",
      "1.8713978763806758 loss\n",
      "0.3017578125\n",
      "1.8473052929722895 loss\n",
      "0.3134765625\n",
      "1.8597634228581077 loss\n",
      "0.3173828125\n",
      "1.8720786629455586 loss\n",
      "0.296875\n",
      "1.8792709049985343 loss\n",
      "0.3125\n",
      "1.9013158496602047 loss\n",
      "0.28125\n",
      "1.871266025400374 loss\n",
      "0.3076171875\n",
      "1.8826409359375862 loss\n",
      "0.2783203125\n",
      "1.8630365609544488 loss\n",
      "0.2919921875\n",
      "1.872529308496933 loss\n",
      "0.2978515625\n",
      "1.829643140568026 loss\n",
      "0.3173828125\n",
      "1.8059279936242099 loss\n",
      "0.3564453125\n",
      "1.8664566682601476 loss\n",
      "0.2998046875\n",
      "1.7922433238890876 loss\n",
      "0.330078125\n",
      "1.8614493720256546 loss\n",
      "0.294921875\n",
      "1.8102634461244484 loss\n",
      "0.33203125\n",
      "1.8533465869358912 loss\n",
      "0.3046875\n",
      "1.843763041658999 loss\n",
      "0.294921875\n",
      "1.8289595142095836 loss\n",
      "0.33203125\n",
      "1.8870112926019953 loss\n",
      "0.26953125\n",
      "1.8071907655488235 loss\n",
      "0.3203125\n",
      "1.8029765465600645 loss\n",
      "0.322265625\n",
      "1.8627743181875411 loss\n",
      "0.322265625\n",
      "1.8360986515070417 loss\n",
      "0.3134765625\n",
      "1.8221252534614618 loss\n",
      "0.3173828125\n",
      "1.8096253613401132 loss\n",
      "0.314453125\n",
      "1.8556434370473593 loss\n",
      "0.3154296875\n",
      "1.8296968051297917 loss\n",
      "0.3359375\n",
      "1.827655169990413 loss\n",
      "0.33967391304347827\n",
      "1.8164609864285142 loss\n",
      "0.3173828125\n",
      "1.8254440900808364 loss\n",
      "0.3095703125\n",
      "1.8679207450906539 loss\n",
      "0.30078125\n",
      "1.79802455394457 loss\n",
      "0.3125\n",
      "1.8101195364866438 loss\n",
      "0.3046875\n",
      "1.8261154760907263 loss\n",
      "0.2958984375\n",
      "1.8215521635986276 loss\n",
      "0.3212890625\n",
      "1.8100971618238977 loss\n",
      "0.302734375\n",
      "1.8317037947421533 loss\n",
      "0.283203125\n",
      "1.786543657246695 loss\n",
      "0.341796875\n",
      "1.8029213277237401 loss\n",
      "0.3232421875\n",
      "1.8159788548535998 loss\n",
      "0.3369140625\n",
      "1.7835047986789534 loss\n",
      "0.3408203125\n",
      "1.7965748617440067 loss\n",
      "0.349609375\n",
      "1.7993682464975818 loss\n",
      "0.298828125\n",
      "1.8172555326691957 loss\n",
      "0.314453125\n",
      "1.8029935441068052 loss\n",
      "0.330078125\n",
      "1.7594011915884484 loss\n",
      "0.333984375\n",
      "1.8026787098699861 loss\n",
      "0.2998046875\n",
      "1.7470689774396186 loss\n",
      "0.33203125\n",
      "1.8033702492833104 loss\n",
      "0.3203125\n",
      "1.8151847358582711 loss\n",
      "0.32421875\n",
      "1.7879777586919203 loss\n",
      "0.3369140625\n",
      "1.7559675817577476 loss\n",
      "0.3232421875\n",
      "1.7717879276112187 loss\n",
      "0.341796875\n",
      "1.7930062519924912 loss\n",
      "0.3095703125\n",
      "1.7773841253630127 loss\n",
      "0.326171875\n",
      "1.7953797737056647 loss\n",
      "0.337890625\n",
      "1.7636280018704311 loss\n",
      "0.3623046875\n",
      "1.7983298157141658 loss\n",
      "0.3154296875\n",
      "1.7544913525228987 loss\n",
      "0.3388671875\n",
      "1.7796071743228188 loss\n",
      "0.328125\n",
      "1.7723061927253856 loss\n",
      "0.33203125\n",
      "1.8031583089154064 loss\n",
      "0.3125\n",
      "1.7412330635327655 loss\n",
      "0.349609375\n",
      "1.7786266874049244 loss\n",
      "0.310546875\n",
      "1.7539684240080804 loss\n",
      "0.33203125\n",
      "1.7609869460506071 loss\n",
      "0.333984375\n",
      "1.741151805038497 loss\n",
      "0.3505859375\n",
      "1.8004125220266731 loss\n",
      "0.326171875\n",
      "1.768454536727262 loss\n",
      "0.337890625\n",
      "1.7409351018497385 loss\n",
      "0.34765625\n",
      "1.7562754942541832 loss\n",
      "0.34375\n",
      "1.767395106223724 loss\n",
      "0.310546875\n",
      "1.7792618016097788 loss\n",
      "0.337890625\n",
      "1.797772674006159 loss\n",
      "0.3291015625\n",
      "1.7638750158279752 loss\n",
      "0.34765625\n",
      "1.7795222754156432 loss\n",
      "0.3056640625\n",
      "1.7639259875770912 loss\n",
      "0.326171875\n",
      "1.7624832567973328 loss\n",
      "0.3203125\n",
      "1.7292340468611611 loss\n",
      "0.330078125\n",
      "1.706978141547565 loss\n",
      "0.3857421875\n",
      "1.7682572551567812 loss\n",
      "0.3271484375\n",
      "1.6899408788404595 loss\n",
      "0.3828125\n",
      "1.7584968103107395 loss\n",
      "0.3388671875\n",
      "1.70938171208812 loss\n",
      "0.3583984375\n",
      "1.7546829493333187 loss\n",
      "0.3251953125\n",
      "1.746536494650699 loss\n",
      "0.3359375\n",
      "1.7305720048667188 loss\n",
      "0.3564453125\n",
      "1.7895811780123914 loss\n",
      "0.302734375\n",
      "1.705943535303095 loss\n",
      "0.3720703125\n",
      "1.7063858530946103 loss\n",
      "0.3603515625\n",
      "1.7652000030399166 loss\n",
      "0.3505859375\n",
      "1.7276477282789733 loss\n",
      "0.365234375\n",
      "1.7253221175269338 loss\n",
      "0.3564453125\n",
      "1.7115510817858075 loss\n",
      "0.3642578125\n",
      "1.7595620576644828 loss\n",
      "0.3525390625\n",
      "1.7368603866677617 loss\n",
      "0.36328125\n",
      "1.7315209195285743 loss\n",
      "0.37228260869565216\n",
      "1.720880920028213 loss\n",
      "0.3505859375\n",
      "1.7328006108795404 loss\n",
      "0.3427734375\n",
      "1.770874155201224 loss\n",
      "0.34375\n",
      "1.7053143727212636 loss\n",
      "0.35546875\n",
      "1.7172601048461418 loss\n",
      "0.357421875\n",
      "1.7285902919719807 loss\n",
      "0.3564453125\n",
      "1.722837517119211 loss\n",
      "0.376953125\n",
      "1.7163288462085977 loss\n",
      "0.3486328125\n",
      "1.7329282087386213 loss\n",
      "0.3134765625\n",
      "1.6916327660628614 loss\n",
      "0.3857421875\n",
      "1.7055769906159939 loss\n",
      "0.34375\n",
      "1.7177623347312378 loss\n",
      "0.3603515625\n",
      "1.6881914486869827 loss\n",
      "0.365234375\n",
      "1.7004346431080308 loss\n",
      "0.388671875\n",
      "1.705820499666725 loss\n",
      "0.3525390625\n",
      "1.7262666954711872 loss\n",
      "0.35546875\n",
      "1.7151877377364633 loss\n",
      "0.3818359375\n",
      "1.6666822211403438 loss\n",
      "0.380859375\n",
      "1.7105338784657727 loss\n",
      "0.34375\n",
      "1.6533657563019557 loss\n",
      "0.3798828125\n",
      "1.715300336277025 loss\n",
      "0.3642578125\n",
      "1.7251039774882908 loss\n",
      "0.3662109375\n",
      "1.7007699479483456 loss\n",
      "0.38671875\n",
      "1.6659583058790837 loss\n",
      "0.3623046875\n",
      "1.6804405667693858 loss\n",
      "0.392578125\n",
      "1.7023337356429282 loss\n",
      "0.365234375\n",
      "1.6836967716818325 loss\n",
      "0.3759765625\n",
      "1.7059863050917328 loss\n",
      "0.37890625\n",
      "1.6736364125558807 loss\n",
      "0.3876953125\n",
      "1.7118612506675512 loss\n",
      "0.3544921875\n",
      "1.6650856427912066 loss\n",
      "0.3828125\n",
      "1.6974372605476737 loss\n",
      "0.384765625\n",
      "1.6850398687567565 loss\n",
      "0.3935546875\n",
      "1.719497035607926 loss\n",
      "0.3642578125\n",
      "1.653387164874843 loss\n",
      "0.3818359375\n",
      "1.6953693068819868 loss\n",
      "0.345703125\n",
      "1.6677245059750674 loss\n",
      "0.3798828125\n",
      "1.6737550441750886 loss\n",
      "0.376953125\n",
      "1.6586257271943992 loss\n",
      "0.3720703125\n",
      "1.7130286851467695 loss\n",
      "0.3876953125\n",
      "1.6876157643931209 loss\n",
      "0.375\n",
      "1.6568440597047738 loss\n",
      "0.4013671875\n",
      "1.67205801846336 loss\n",
      "0.396484375\n",
      "1.6848999207995543 loss\n",
      "0.3603515625\n",
      "1.6959496829791134 loss\n",
      "0.38671875\n",
      "1.7143589454417376 loss\n",
      "0.3779296875\n",
      "1.67831585205322 loss\n",
      "0.396484375\n",
      "1.694711552116814 loss\n",
      "0.3740234375\n",
      "1.6852951596103292 loss\n",
      "0.369140625\n",
      "1.6709693287126481 loss\n",
      "0.396484375\n",
      "1.6450744809251046 loss\n",
      "0.3935546875\n",
      "1.6246605012989126 loss\n",
      "0.4169921875\n",
      "1.690003212732435 loss\n",
      "0.380859375\n",
      "1.606176928188946 loss\n",
      "0.431640625\n",
      "1.6734546241443176 loss\n",
      "0.392578125\n",
      "1.6290681317242979 loss\n",
      "0.4111328125\n",
      "1.6729154088395237 loss\n",
      "0.3828125\n",
      "1.6673921265035831 loss\n",
      "0.3916015625\n",
      "1.6535253508504462 loss\n",
      "0.396484375\n",
      "1.709426579609714 loss\n",
      "0.3427734375\n",
      "1.6263383643937435 loss\n",
      "0.4326171875\n",
      "1.6299041889980195 loss\n",
      "0.3935546875\n",
      "1.685242124233283 loss\n",
      "0.396484375\n",
      "1.6417563921615421 loss\n",
      "0.4169921875\n",
      "1.6447893400394387 loss\n",
      "0.404296875\n",
      "1.6321261500834594 loss\n",
      "0.3896484375\n",
      "1.6804056095519555 loss\n",
      "0.3876953125\n",
      "1.6643224910050334 loss\n",
      "0.384765625\n",
      "1.6515792360708093 loss\n",
      "0.38858695652173914\n",
      "1.644743873645754 loss\n",
      "0.3623046875\n",
      "1.6541441412398 loss\n",
      "0.3740234375\n",
      "1.6922798486919128 loss\n",
      "0.375\n",
      "1.6296770239017109 loss\n",
      "0.3818359375\n",
      "1.6384731500695995 loss\n",
      "0.4072265625\n",
      "1.6484677932095122 loss\n",
      "0.3701171875\n",
      "1.6428903438609004 loss\n",
      "0.3994140625\n",
      "1.640421478609788 loss\n",
      "0.369140625\n",
      "1.6531035718713418 loss\n",
      "0.376953125\n",
      "1.613109401289028 loss\n",
      "0.40625\n",
      "1.62516380316971 loss\n",
      "0.3818359375\n",
      "1.6376202421903985 loss\n",
      "0.39453125\n",
      "1.6105548455491632 loss\n",
      "0.4140625\n",
      "1.6232504594614305 loss\n",
      "0.427734375\n",
      "1.6284649580285326 loss\n",
      "0.400390625\n",
      "1.651375830759329 loss\n",
      "0.384765625\n",
      "1.641466534604621 loss\n",
      "0.3916015625\n",
      "1.589980560628882 loss\n",
      "0.404296875\n",
      "1.634157158232924 loss\n",
      "0.3681640625\n",
      "1.5777326153006106 loss\n",
      "0.390625\n",
      "1.640839665227555 loss\n",
      "0.396484375\n",
      "1.6495517992875255 loss\n",
      "0.400390625\n",
      "1.6300221686338965 loss\n",
      "0.3974609375\n",
      "1.5904109647848799 loss\n",
      "0.3857421875\n",
      "1.6038574054628483 loss\n",
      "0.4189453125\n",
      "1.6275071733146094 loss\n",
      "0.4013671875\n",
      "1.6050592157416872 loss\n",
      "0.4072265625\n",
      "1.6310622871476173 loss\n",
      "0.3935546875\n",
      "1.598493762875284 loss\n",
      "0.4150390625\n",
      "1.6381475124278555 loss\n",
      "0.3740234375\n",
      "1.5897144626371746 loss\n",
      "0.404296875\n",
      "1.627408294607681 loss\n",
      "0.4140625\n",
      "1.610091175951189 loss\n",
      "0.4140625\n",
      "1.6471388885962752 loss\n",
      "0.39453125\n",
      "1.5789260907159308 loss\n",
      "0.40625\n",
      "1.6245942580777284 loss\n",
      "0.3681640625\n",
      "1.592447238566086 loss\n",
      "0.4072265625\n",
      "1.599608738653626 loss\n",
      "0.41015625\n",
      "1.5898450675661073 loss\n",
      "0.3984375\n",
      "1.638083605475629 loss\n",
      "0.4072265625\n",
      "1.618636290315768 loss\n",
      "0.408203125\n",
      "1.5854791697196384 loss\n",
      "0.4228515625\n",
      "1.598206385595307 loss\n",
      "0.40234375\n",
      "1.6133173868205184 loss\n",
      "0.39453125\n",
      "1.6225070835184705 loss\n",
      "0.4033203125\n",
      "1.6410204055619415 loss\n",
      "0.4052734375\n",
      "1.6049166928983947 loss\n",
      "0.4033203125\n",
      "1.6200850189338354 loss\n",
      "0.3974609375\n",
      "1.6168823292649026 loss\n",
      "0.3994140625\n",
      "1.592848246979542 loss\n",
      "0.427734375\n",
      "1.570630106691946 loss\n",
      "0.42578125\n",
      "1.5516430580740321 loss\n",
      "0.43359375\n",
      "1.6207641589053616 loss\n",
      "0.416015625\n",
      "1.5321471052027338 loss\n",
      "0.451171875\n",
      "1.5973592519925244 loss\n",
      "0.4130859375\n",
      "1.559928264732862 loss\n",
      "0.4375\n",
      "1.597722771316354 loss\n",
      "0.4111328125\n",
      "1.5967322679681728 loss\n",
      "0.4130859375\n",
      "1.5854473214640945 loss\n",
      "0.4345703125\n",
      "1.6374169459057613 loss\n",
      "0.3896484375\n",
      "1.5555367851548885 loss\n",
      "0.462890625\n",
      "1.5620045059410521 loss\n",
      "0.4140625\n",
      "1.612116742684407 loss\n",
      "0.431640625\n",
      "1.5658279935305082 loss\n",
      "0.443359375\n",
      "1.5710974185757562 loss\n",
      "0.43359375\n",
      "1.5603188286275729 loss\n",
      "0.4267578125\n",
      "1.6079659257697674 loss\n",
      "0.416015625\n",
      "1.5995806492786673 loss\n",
      "0.416015625\n",
      "1.5762752089858212 loss\n",
      "0.43478260869565216\n",
      "1.5765782146398637 loss\n",
      "0.404296875\n",
      "1.580629189108203 loss\n",
      "0.40234375\n",
      "1.6208423989847072 loss\n",
      "0.41015625\n",
      "1.5603160181205957 loss\n",
      "0.4208984375\n",
      "1.5648616552549142 loss\n",
      "0.4365234375\n",
      "1.574300151190998 loss\n",
      "0.40234375\n",
      "1.5706618477770833 loss\n",
      "0.4326171875\n",
      "1.5711616714827987 loss\n",
      "0.4013671875\n",
      "1.5808683116897173 loss\n",
      "0.40625\n",
      "1.5398849448666905 loss\n",
      "0.4453125\n",
      "1.54995363360763 loss\n",
      "0.427734375\n",
      "1.5641599769791785 loss\n",
      "0.4296875\n",
      "1.5386371581731741 loss\n",
      "0.4521484375\n",
      "1.5515106967469576 loss\n",
      "0.462890625\n",
      "1.555657868828404 loss\n",
      "0.4326171875\n",
      "1.5796103934489292 loss\n",
      "0.427734375\n",
      "1.569992671938535 loss\n",
      "0.4423828125\n",
      "1.5171672635146978 loss\n",
      "0.45703125\n",
      "1.56340965848679 loss\n",
      "0.400390625\n",
      "1.5077644243735309 loss\n",
      "0.4384765625\n",
      "1.5702311930918884 loss\n",
      "0.4326171875\n",
      "1.577352123745327 loss\n",
      "0.431640625\n",
      "1.563553461439791 loss\n",
      "0.4228515625\n",
      "1.5178001730144588 loss\n",
      "0.4375\n",
      "1.5312746136380206 loss\n",
      "0.46484375\n",
      "1.5567281356993985 loss\n",
      "0.4443359375\n",
      "1.5302129195637861 loss\n",
      "0.443359375\n",
      "1.5589237933329914 loss\n",
      "0.4375\n",
      "1.5267595589129446 loss\n",
      "0.4501953125\n",
      "1.5662682301852846 loss\n",
      "0.421875\n",
      "1.516457963309036 loss\n",
      "0.4541015625\n",
      "1.5595392702592963 loss\n",
      "0.4560546875\n",
      "1.537118550001968 loss\n",
      "0.447265625\n",
      "1.5750398318362842 loss\n",
      "0.42578125\n",
      "1.5060552843897068 loss\n",
      "0.4560546875\n",
      "1.5553730385079994 loss\n",
      "0.4150390625\n",
      "1.5187625480129432 loss\n",
      "0.455078125\n",
      "1.5268797279890989 loss\n",
      "0.44921875\n",
      "1.5229023227698684 loss\n",
      "0.44140625\n",
      "1.564995105525877 loss\n",
      "0.4306640625\n",
      "1.55015742534682 loss\n",
      "0.4404296875\n",
      "1.514920689052742 loss\n",
      "0.4638671875\n",
      "1.5233073398604102 loss\n",
      "0.4462890625\n",
      "1.5412415531203771 loss\n",
      "0.4404296875\n",
      "1.5490856633752608 loss\n",
      "0.4287109375\n",
      "1.5679543004809284 loss\n",
      "0.43359375\n",
      "1.533155545622588 loss\n",
      "0.4404296875\n",
      "1.5450158961693967 loss\n",
      "0.4345703125\n",
      "1.547345071025748 loss\n",
      "0.43359375\n",
      "1.5179731645821506 loss\n",
      "0.462890625\n",
      "1.496321618856759 loss\n",
      "0.46484375\n",
      "1.4799371488048725 loss\n",
      "0.4658203125\n",
      "1.5502712909044205 loss\n",
      "0.451171875\n",
      "1.4588534260737949 loss\n",
      "0.494140625\n",
      "1.520602439559282 loss\n",
      "0.4501953125\n",
      "1.491691726801288 loss\n",
      "0.4697265625\n",
      "1.5198222955824279 loss\n",
      "0.447265625\n",
      "1.5249487670538096 loss\n",
      "0.4384765625\n",
      "1.515893123914136 loss\n",
      "0.4677734375\n",
      "1.5636646811171664 loss\n",
      "0.4267578125\n",
      "1.4825875757403626 loss\n",
      "0.490234375\n",
      "1.492759020058619 loss\n",
      "0.4580078125\n",
      "1.5361364057900748 loss\n",
      "0.462890625\n",
      "1.4887481760158914 loss\n",
      "0.4853515625\n",
      "1.4951715524780516 loss\n",
      "0.46875\n",
      "1.4871123574645668 loss\n",
      "0.4560546875\n",
      "1.5340099804112595 loss\n",
      "0.451171875\n",
      "1.5327001102552804 loss\n",
      "0.451171875\n",
      "1.4975883432179597 loss\n",
      "0.4592391304347826\n",
      "1.5067377256753889 loss\n",
      "0.4501953125\n",
      "1.5039699858797668 loss\n",
      "0.4443359375\n",
      "1.5473896341809654 loss\n",
      "0.451171875\n",
      "1.4890783216454344 loss\n",
      "0.4697265625\n",
      "1.4885051019944193 loss\n",
      "0.4736328125\n",
      "1.4971487619178405 loss\n",
      "0.44140625\n",
      "1.49745818411974 loss\n",
      "0.46484375\n",
      "1.5001255709314574 loss\n",
      "0.447265625\n",
      "1.507216406656982 loss\n",
      "0.4482421875\n",
      "1.4640283924485127 loss\n",
      "0.505859375\n",
      "1.4721598613503428 loss\n",
      "0.474609375\n",
      "1.4890426668577288 loss\n",
      "0.4609375\n",
      "1.4636636922479431 loss\n",
      "0.4853515625\n",
      "1.4763188193416736 loss\n",
      "0.5048828125\n",
      "1.4794031663031162 loss\n",
      "0.48046875\n",
      "1.503016152627715 loss\n",
      "0.4619140625\n",
      "1.4935299683781738 loss\n",
      "0.4892578125\n",
      "1.4407647707801463 loss\n",
      "0.490234375\n",
      "1.4913529605677944 loss\n",
      "0.443359375\n",
      "1.4359975932887887 loss\n",
      "0.4814453125\n",
      "1.4974468643490124 loss\n",
      "0.46875\n",
      "1.5008590290926382 loss\n",
      "0.4716796875\n",
      "1.494280336356223 loss\n",
      "0.455078125\n",
      "1.4408049982491236 loss\n",
      "0.4951171875\n",
      "1.4565088620906756 loss\n",
      "0.5048828125\n",
      "1.482384353408428 loss\n",
      "0.4775390625\n",
      "1.452179619865019 loss\n",
      "0.490234375\n",
      "1.4826507030981844 loss\n",
      "0.4794921875\n",
      "1.4525360650313193 loss\n",
      "0.4873046875\n",
      "1.4905665121068574 loss\n",
      "0.470703125\n",
      "1.4401502314034773 loss\n",
      "0.501953125\n",
      "1.4885429560735925 loss\n",
      "0.4921875\n",
      "1.4608334851931144 loss\n",
      "0.4873046875\n",
      "1.498146544247404 loss\n",
      "0.46484375\n",
      "1.4291764028553335 loss\n",
      "0.498046875\n",
      "1.4827460030549058 loss\n",
      "0.4619140625\n",
      "1.4423847473898437 loss\n",
      "0.4951171875\n",
      "1.4499122212047186 loss\n",
      "0.4833984375\n",
      "1.4524055689984499 loss\n",
      "0.48828125\n",
      "1.4889938287741349 loss\n",
      "0.458984375\n",
      "1.477553884248691 loss\n",
      "0.4736328125\n",
      "1.4401955053281983 loss\n",
      "0.505859375\n",
      "1.4432885445155144 loss\n",
      "0.4833984375\n",
      "1.4644313946027814 loss\n",
      "0.478515625\n",
      "1.4722021667255856 loss\n",
      "0.47265625\n",
      "1.491883138514614 loss\n",
      "0.4697265625\n",
      "1.45847137936902 loss\n",
      "0.474609375\n",
      "1.4662956693457234 loss\n",
      "0.48046875\n",
      "1.4730290055369195 loss\n",
      "0.484375\n",
      "1.4419134135760412 loss\n",
      "0.4951171875\n",
      "1.419293295296561 loss\n",
      "0.5029296875\n",
      "1.4064334227360367 loss\n",
      "0.4921875\n",
      "1.475029565174849 loss\n",
      "0.4951171875\n",
      "1.383723596160811 loss\n",
      "0.51953125\n",
      "1.4403406085585253 loss\n",
      "0.486328125\n",
      "1.4212356105212662 loss\n",
      "0.5068359375\n",
      "1.4376346378907934 loss\n",
      "0.486328125\n",
      "1.4505458108831282 loss\n",
      "0.4736328125\n",
      "1.4427220125499969 loss\n",
      "0.490234375\n",
      "1.485385897516907 loss\n",
      "0.470703125\n",
      "1.4058151907732928 loss\n",
      "0.525390625\n",
      "1.4195321159750862 loss\n",
      "0.4912109375\n",
      "1.4562832893928594 loss\n",
      "0.50390625\n",
      "1.4090058558091738 loss\n",
      "0.525390625\n",
      "1.4154784628903934 loss\n",
      "0.5\n",
      "1.41150653890535 loss\n",
      "0.49609375\n",
      "1.4568958181111913 loss\n",
      "0.4873046875\n",
      "1.4618049446086778 loss\n",
      "0.4970703125\n",
      "1.4165667670860436 loss\n",
      "0.4945652173913043\n",
      "1.4339579627262466 loss\n",
      "0.4931640625\n",
      "1.4240067976766755 loss\n",
      "0.486328125\n",
      "1.4709557757457057 loss\n",
      "0.478515625\n",
      "1.4150784355320085 loss\n",
      "0.5107421875\n",
      "1.4085647895983875 loss\n",
      "0.5126953125\n",
      "1.417200412391737 loss\n",
      "0.4853515625\n",
      "1.423468343836762 loss\n",
      "0.5029296875\n",
      "1.4262105399675828 loss\n",
      "0.4853515625\n",
      "1.431595046454267 loss\n",
      "0.4794921875\n",
      "1.3859568610980322 loss\n",
      "0.548828125\n",
      "1.3922725492663541 loss\n",
      "0.5087890625\n",
      "1.4117270615984527 loss\n",
      "0.4921875\n",
      "1.3857289167886946 loss\n",
      "0.5078125\n",
      "1.3979257308125463 loss\n",
      "0.525390625\n",
      "1.4004350489101434 loss\n",
      "0.513671875\n",
      "1.4226779246541075 loss\n",
      "0.5\n",
      "1.4149984895151795 loss\n",
      "0.5234375\n",
      "1.361956822417974 loss\n",
      "0.52734375\n",
      "1.417978130683411 loss\n",
      "0.4873046875\n",
      "1.3631220888155196 loss\n",
      "0.513671875\n",
      "1.4230172294978645 loss\n",
      "0.494140625\n",
      "1.4218321761508912 loss\n",
      "0.501953125\n",
      "1.4225874001212195 loss\n",
      "0.505859375\n",
      "1.3614998427399287 loss\n",
      "0.5390625\n",
      "1.3807364154379869 loss\n",
      "0.5419921875\n",
      "1.4056813941961395 loss\n",
      "0.51171875\n",
      "1.3738293700611195 loss\n",
      "0.5283203125\n",
      "1.4043860908689463 loss\n",
      "0.5068359375\n",
      "1.3780663639556892 loss\n",
      "0.521484375\n",
      "1.4134906709217194 loss\n",
      "0.50390625\n",
      "1.3631296022750266 loss\n",
      "0.541015625\n",
      "1.4161415969907878 loss\n",
      "0.5283203125\n",
      "1.3836378313152329 loss\n",
      "0.52734375\n",
      "1.4198634667557917 loss\n",
      "0.5029296875\n",
      "1.3507670795950695 loss\n",
      "0.53125\n",
      "1.4086322973931464 loss\n",
      "0.4990234375\n",
      "1.3651916315103647 loss\n",
      "0.5458984375\n",
      "1.3715053316819694 loss\n",
      "0.517578125\n",
      "1.3809485126329193 loss\n",
      "0.5234375\n",
      "1.4130139638787993 loss\n",
      "0.498046875\n",
      "1.4028830405084358 loss\n",
      "0.4970703125\n",
      "1.3635825891089444 loss\n",
      "0.5341796875\n",
      "1.3623270596750654 loss\n",
      "0.53515625\n",
      "1.3859723499040633 loss\n",
      "0.5166015625\n",
      "1.3943525486069954 loss\n",
      "0.517578125\n",
      "1.4166632774456414 loss\n",
      "0.5\n",
      "1.3842094907367355 loss\n",
      "0.517578125\n",
      "1.3883716904322394 loss\n",
      "0.4990234375\n",
      "1.3974996335410204 loss\n",
      "0.5224609375\n",
      "1.3670817850921364 loss\n",
      "0.5205078125\n",
      "1.3423633897868 loss\n",
      "0.54296875\n",
      "1.3342514426506438 loss\n",
      "0.5263671875\n",
      "1.3984421525560582 loss\n",
      "0.5244140625\n",
      "1.3097691767036572 loss\n",
      "0.552734375\n",
      "1.3602046995542711 loss\n",
      "0.5185546875\n",
      "1.351037859649162 loss\n",
      "0.5380859375\n",
      "1.3560215573188847 loss\n",
      "0.5244140625\n",
      "1.3776666124046122 loss\n",
      "0.50390625\n",
      "1.3685097691533015 loss\n",
      "0.5224609375\n",
      "1.406371355165964 loss\n",
      "0.5087890625\n",
      "1.3295041928279405 loss\n",
      "0.54296875\n",
      "1.3450139165812156 loss\n",
      "0.5283203125\n",
      "1.3774381255603103 loss\n",
      "0.5439453125\n",
      "1.3299660129732505 loss\n",
      "0.55859375\n",
      "1.3355262511260184 loss\n",
      "0.5380859375\n",
      "1.3368441498880808 loss\n",
      "0.537109375\n",
      "1.3807765276427368 loss\n",
      "0.521484375\n",
      "1.389972444338278 loss\n",
      "0.5322265625\n",
      "1.3388254461007971 loss\n",
      "0.5244565217391305\n",
      "1.3624151470541452 loss\n",
      "0.5234375\n",
      "1.3455120862070395 loss\n",
      "0.533203125\n",
      "1.3953953425035333 loss\n",
      "0.5166015625\n",
      "1.341493585207827 loss\n",
      "0.5400390625\n",
      "1.3292153835971812 loss\n",
      "0.544921875\n",
      "1.3392262609763987 loss\n",
      "0.5224609375\n",
      "1.352489412010013 loss\n",
      "0.5361328125\n",
      "1.352478932361218 loss\n",
      "0.51953125\n",
      "1.3578453472393512 loss\n",
      "0.525390625\n",
      "1.3103825246619734 loss\n",
      "0.572265625\n",
      "1.3143946593733045 loss\n",
      "0.548828125\n",
      "1.3356132306076027 loss\n",
      "0.53125\n",
      "1.3094743077658197 loss\n",
      "0.5380859375\n",
      "1.3212793627111383 loss\n",
      "0.572265625\n",
      "1.322931419596091 loss\n",
      "0.55078125\n",
      "1.3437227428835918 loss\n",
      "0.55078125\n",
      "1.3391055508363925 loss\n",
      "0.5576171875\n",
      "1.2844682250549093 loss\n",
      "0.5576171875\n",
      "1.3463855486655805 loss\n",
      "0.5283203125\n",
      "1.292617727549567 loss\n",
      "0.5517578125\n",
      "1.3499343475778522 loss\n",
      "0.5224609375\n",
      "1.3452072908651964 loss\n",
      "0.546875\n",
      "1.3516665727359154 loss\n",
      "0.54296875\n",
      "1.2841079439684282 loss\n",
      "0.568359375\n",
      "1.3074721068123007 loss\n",
      "0.5693359375\n",
      "1.3303205640658433 loss\n",
      "0.5517578125\n",
      "1.299249109651135 loss\n",
      "0.568359375\n",
      "1.3281591109055277 loss\n",
      "0.546875\n",
      "1.3073936096719136 loss\n",
      "0.55078125\n",
      "1.3392031304569614 loss\n",
      "0.546875\n",
      "1.2885693573490873 loss\n",
      "0.5751953125\n",
      "1.3456103216118982 loss\n",
      "0.57421875\n",
      "1.3089201207114232 loss\n",
      "0.5546875\n",
      "1.3438287446009363 loss\n",
      "0.537109375\n",
      "1.2746752476997187 loss\n",
      "0.5712890625\n",
      "1.336839172092254 loss\n",
      "0.5380859375\n",
      "1.2904835559072956 loss\n",
      "0.5791015625\n",
      "1.2950398449434484 loss\n",
      "0.5576171875\n",
      "1.3113806276232038 loss\n",
      "0.5654296875\n",
      "1.3401418486102146 loss\n",
      "0.533203125\n",
      "1.3289351279859167 loss\n",
      "0.5380859375\n",
      "1.2883081016075777 loss\n",
      "0.5693359375\n",
      "1.2842713881861547 loss\n",
      "0.5703125\n",
      "1.309333684425844 loss\n",
      "0.564453125\n",
      "1.3182156129707412 loss\n",
      "0.546875\n",
      "1.3449607161917312 loss\n",
      "0.53515625\n",
      "1.3135831275450247 loss\n",
      "0.5419921875\n",
      "1.3152389777366253 loss\n",
      "0.53125\n",
      "1.3238817714807332 loss\n",
      "0.5537109375\n",
      "1.295729978103162 loss\n",
      "0.5546875\n",
      "1.2684633762532935 loss\n",
      "0.5732421875\n",
      "1.2657036745587897 loss\n",
      "0.560546875\n",
      "1.3235774863354963 loss\n",
      "0.552734375\n",
      "1.239447375270684 loss\n",
      "0.5888671875\n",
      "1.2825579580075748 loss\n",
      "0.546875\n",
      "1.2829924935659807 loss\n",
      "0.552734375\n",
      "1.2787549562641387 loss\n",
      "0.56640625\n",
      "1.3083810849314494 loss\n",
      "0.541015625\n",
      "1.2952309469385916 loss\n",
      "0.5673828125\n",
      "1.3294446487795124 loss\n",
      "0.5390625\n",
      "1.2566209584799568 loss\n",
      "0.5830078125\n",
      "1.272186608856329 loss\n",
      "0.5654296875\n",
      "1.3029630420939167 loss\n",
      "0.5751953125\n",
      "1.254045286326517 loss\n",
      "0.5888671875\n",
      "1.257595674412713 loss\n",
      "0.568359375\n",
      "1.2653414911277883 loss\n",
      "0.5712890625\n",
      "1.3080390629213117 loss\n",
      "0.548828125\n",
      "1.3189873014104911 loss\n",
      "0.564453125\n",
      "1.2662633640254446 loss\n",
      "0.5652173913043478\n",
      "1.2940486722513467 loss\n",
      "0.5537109375\n",
      "1.271246953076706 loss\n",
      "0.56640625\n",
      "1.3225598678346082 loss\n",
      "0.5556640625\n",
      "1.2700232654778183 loss\n",
      "0.560546875\n",
      "1.2522152972581413 loss\n",
      "0.5771484375\n",
      "1.2656624002221664 loss\n",
      "0.564453125\n",
      "1.2857309683856064 loss\n",
      "0.564453125\n",
      "1.28044789067363 loss\n",
      "0.548828125\n",
      "1.288046572296131 loss\n",
      "0.564453125\n",
      "1.2394479907436304 loss\n",
      "0.5849609375\n",
      "1.240049214336472 loss\n",
      "0.58203125\n",
      "1.262300657091683 loss\n",
      "0.568359375\n",
      "1.2366233885032396 loss\n",
      "0.5654296875\n",
      "1.248512548198997 loss\n",
      "0.6015625\n",
      "1.248821974735022 loss\n",
      "0.583984375\n",
      "1.268147772515864 loss\n",
      "0.5849609375\n",
      "1.2670719239948876 loss\n",
      "0.58984375\n",
      "1.209387651000505 loss\n",
      "0.591796875\n",
      "1.2773931225682733 loss\n",
      "0.5625\n",
      "1.2254186117085237 loss\n",
      "0.58984375\n",
      "1.279195480671706 loss\n",
      "0.55859375\n",
      "1.27166230822237 loss\n",
      "0.57421875\n",
      "1.2820624080964664 loss\n",
      "0.58203125\n",
      "1.210151735083537 loss\n",
      "0.599609375\n",
      "1.2382211582565879 loss\n",
      "0.60546875\n",
      "1.2570097241009566 loss\n",
      "0.58203125\n",
      "1.22819085592198 loss\n",
      "0.59375\n",
      "1.2549166077056886 loss\n",
      "0.578125\n",
      "1.2409711234192526 loss\n",
      "0.58203125\n",
      "1.2680871261725817 loss\n",
      "0.578125\n",
      "1.2170969474055253 loss\n",
      "0.5966796875\n",
      "1.2767961071424283 loss\n",
      "0.6044921875\n",
      "1.2373642927422699 loss\n",
      "0.5927734375\n",
      "1.270469540761286 loss\n",
      "0.5703125\n",
      "1.2015525356193308 loss\n",
      "0.609375\n",
      "1.2680213741729283 loss\n",
      "0.5703125\n",
      "1.2186323869454796 loss\n",
      "0.599609375\n",
      "1.2207251946766866 loss\n",
      "0.5869140625\n",
      "1.2438284088546487 loss\n",
      "0.5947265625\n",
      "1.2698669358698895 loss\n",
      "0.568359375\n",
      "1.2568699902045326 loss\n",
      "0.5791015625\n",
      "1.2157621001159329 loss\n",
      "0.5966796875\n",
      "1.2098291686859441 loss\n",
      "0.607421875\n",
      "1.2354076590432523 loss\n",
      "0.5966796875\n",
      "1.24419005170156 loss\n",
      "0.5771484375\n",
      "1.2765602143295878 loss\n",
      "0.56640625\n",
      "1.2463484238135254 loss\n",
      "0.5732421875\n",
      "1.2467434161727446 loss\n",
      "0.56640625\n",
      "1.2521890587492024 loss\n",
      "0.578125\n",
      "1.2269776287824838 loss\n",
      "0.5927734375\n",
      "1.197303092307108 loss\n",
      "0.6162109375\n",
      "1.2005015493928353 loss\n",
      "0.5908203125\n",
      "1.2510162393091775 loss\n",
      "0.5986328125\n",
      "1.1728505502890714 loss\n",
      "0.619140625\n",
      "1.2078952280194217 loss\n",
      "0.587890625\n",
      "1.2170035256606762 loss\n",
      "0.58984375\n",
      "1.2057216425855377 loss\n",
      "0.5966796875\n",
      "1.2417321858073014 loss\n",
      "0.572265625\n",
      "1.22286388578809 loss\n",
      "0.609375\n",
      "1.2546452050685872 loss\n",
      "0.57421875\n",
      "1.186849265220562 loss\n",
      "0.6162109375\n",
      "1.2019758687775612 loss\n",
      "0.59765625\n",
      "1.2324949778653829 loss\n",
      "0.609375\n",
      "1.180952759248162 loss\n",
      "0.6123046875\n",
      "1.1815847658557577 loss\n",
      "0.6123046875\n",
      "1.1963273493021926 loss\n",
      "0.603515625\n",
      "1.2380345845903444 loss\n",
      "0.578125\n",
      "1.2491570792446296 loss\n",
      "0.599609375\n",
      "1.1969254635947346 loss\n",
      "0.6005434782608695\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    for j in range(0, len(X), batch_size):\n",
    "#     for j in range(0, 5 * batch_size, batch_size):\n",
    "\n",
    "        X_batch = X[j: j + batch_size]\n",
    "        y_batch = y[j: j + batch_size]\n",
    "        y_, loss = mlp.forward(X_batch, y_batch)\n",
    "        mlp.backward(y_, y_batch)\n",
    "        mlp.step(lr=0.01)\n",
    "        if j % batch_size * 8 == 0:\n",
    "#             print(j)\n",
    "            print(loss, 'loss')\n",
    "            print((np.argmax(y_, axis=-1) == np.argmax(y_batch, axis=-1)).sum() / len(y_batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
