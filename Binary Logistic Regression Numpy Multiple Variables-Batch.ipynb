{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = load_breast_cancer(return_X_y=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = StandardScaler().fit_transform(X)\n",
    "y = y[..., np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear:\n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        self.w = np.random.randn(in_dim, out_dim)\n",
    "        self.b = np.zeros([out_dim])\n",
    "        self.dw = None\n",
    "        self.db = None\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return np.matmul(x, self.w) + self.b\n",
    "    \n",
    "    def backward(self, d, x):\n",
    "        self.dw = np.matmul(x.T, d)\n",
    "        assert self.dw.shape == self.w.shape\n",
    "        self.db = d\n",
    "        \n",
    "    def step(self, lr):\n",
    "        self.w = self.w - lr * self.dw\n",
    "        self.b = self.b - lr * self.db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid:\n",
    "    def __init__(self):\n",
    "        self.a = None\n",
    "    def forward(self, x):\n",
    "        self.a = 1 / (1 + np.exp(-x))\n",
    "        return self.a\n",
    "    def backward(self, d):\n",
    "        return d * (self.a * (1 - self.a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinaryCrossEntropy:\n",
    "    def __init__(self, average=True): \n",
    "        self.average = average\n",
    "    def forward(self, y_, y):\n",
    "        assert y.shape == y_.shape, (y.shape, y_.shape)\n",
    "        l = np.sum(-(y * np.log(y_ + 1e-9) + (1 - y) * np.log(1 - y_ + 1e-9)))\n",
    "        if self.average:\n",
    "            l /= len(y)\n",
    "        return y_, l\n",
    "    \n",
    "    def backward(self, y_, y):\n",
    "        assert y_.shape == y.shape\n",
    "        d = - y / (y_ + 1e-9) + (1 - y) / (1 - y_ + 1e-9)\n",
    "        if self.average:\n",
    "            d /= len(d)\n",
    "        return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression:\n",
    "    def __init__(self):\n",
    "        self.linear = Linear(30, 1)\n",
    "        self.sigmoid = Sigmoid()\n",
    "        self.loss = BinaryCrossEntropy()\n",
    "        \n",
    "    def forward(self, x, y):\n",
    "        x = self.linear.forward(x)\n",
    "        x = self.sigmoid.forward(x)\n",
    "        loss = self.loss.forward(x, y)\n",
    "        return loss\n",
    "    \n",
    "    def backward(self, x, y_, y):\n",
    "        d = self.loss.backward(y_, y)\n",
    "        d = self.sigmoid.backward(d)\n",
    "        self.linear.backward(d, x)\n",
    "    \n",
    "    def step(self, lr):\n",
    "        self.linear.step(lr)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.764371657111462 loss\n",
      "0.16187791849347682 loss\n",
      "0.1164361775375034 loss\n",
      "0.09793039675395478 loss\n",
      "0.08794068041917709 loss\n",
      "0.08135860686252083 loss\n",
      "0.07655375043225707 loss\n",
      "0.07287217291400144 loss\n",
      "0.06996651590401651 loss\n",
      "0.06761050683977615 loss\n",
      "0.06564762489191951 loss\n",
      "0.06397154758702212 loss\n",
      "0.06251079380296753 loss\n",
      "0.06121652366465179 loss\n",
      "0.06005436224868867 loss\n",
      "0.058999339524727336 loss\n",
      "0.05803278885454969 loss\n",
      "0.05714040543921006 loss\n",
      "0.05631099352380575 loss\n",
      "0.05553563261926218 loss\n",
      "0.05480710628970402 loss\n",
      "0.05411950035599759 loss\n",
      "0.05346791345458243 loss\n",
      "0.05284824402598071 loss\n",
      "0.05225703052608319 loss\n",
      "0.05169132950598677 loss\n",
      "0.05114862117190024 loss\n",
      "0.05062673524792082 loss\n",
      "0.050123792085411976 loss\n",
      "0.049638155391696574 loss\n",
      "0.04916839393183863 loss\n",
      "0.048713250242907624 loss\n",
      "0.048271614887344334 loss\n",
      "0.04784250512375939 loss\n",
      "0.04742504713110299 loss\n",
      "0.047018461113430006 loss\n",
      "0.046622048756286755 loss\n",
      "0.04623518261513166 loss\n",
      "0.04585729710027811 loss\n",
      "0.04548788078811909 loss\n",
      "0.04512646983949818 loss\n",
      "0.044772642346457635 loss\n",
      "0.04442601346069187 loss\n",
      "0.04408623118275392 loss\n",
      "0.043752972711788705 loss\n",
      "0.043425941272366264 loss\n",
      "0.043104863348687655 loss\n",
      "0.04278948626765446 loss\n",
      "0.04247957608151787 loss\n",
      "0.04217491570845957 loss\n",
      "0.04187530329577864 loss\n",
      "0.041580550775634544 loss\n",
      "0.04129048258769869 loss\n",
      "0.041004934546763794 loss\n",
      "0.04072375283646445 loss\n",
      "0.04044679311289077 loss\n",
      "0.0401739197040903 loss\n",
      "0.039905004893344795 loss\n",
      "0.03963992827570905 loss\n",
      "0.039378576178675524 loss\n",
      "0.039120841139002056 loss\n",
      "0.0388666214287557 loss\n",
      "0.038615820624502556 loss\n",
      "0.03836834721433549 loss\n",
      "0.03812411423809983 loss\n",
      "0.037883038956765015 loss\n",
      "0.03764504254741232 loss\n",
      "0.0374100498207773 loss\n",
      "0.03717798895870469 loss\n",
      "0.03694879126926168 loss\n",
      "0.0367223909576035 loss\n",
      "0.03649872491101271 loss\n",
      "0.03627773249682962 loss\n",
      "0.03605935537227148 loss\n",
      "0.035843537305387516 loss\n",
      "0.03563022400662871 loss\n",
      "0.035419362970715254 loss\n",
      "0.035210903328658616 loss\n",
      "0.035004795709944324 loss\n",
      "0.03480099211498951 loss\n",
      "0.03459944579806799 loss\n",
      "0.034400111160931335 loss\n",
      "0.03420294365735322 loss\n",
      "0.034007899708784754 loss\n",
      "0.03381493663123109 loss\n",
      "0.03362401257335105 loss\n",
      "0.03343508646564508 loss\n",
      "0.033248117980441715 loss\n",
      "0.033063067502223785 loss\n",
      "0.03287989610766751 loss\n",
      "0.032698565554604465 loss\n",
      "0.03251903827896841 loss\n",
      "0.03234127739866851 loss\n",
      "0.032165246723236236 loss\n",
      "0.03199091076803699 loss\n",
      "0.03181823477181775 loss\n",
      "0.03164718471637959 loss\n",
      "0.03147772734721873 loss\n",
      "0.031309830194063464 loss\n",
      "0.03114346159034948 loss\n"
     ]
    }
   ],
   "source": [
    "for i in range(10000):\n",
    "    y_, loss = logreg.forward(X, y)\n",
    "    logreg.backward(X, y_, y)\n",
    "    logreg.step(0.1)\n",
    "    if i % 100 == 0:\n",
    "        print(loss, 'loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
