{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = load_digits(return_X_y=True)\n",
    "y = np.eye(10)[y].astype(np.float32)\n",
    "X = StandardScaler().fit_transform(X)\n",
    "X = X.reshape([1797, 1, 8, 8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_n = X.copy()\n",
    "t_n = y.copy()[:, None, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv:\n",
    "    def __init__(self, channels_in, channels_out, k):\n",
    "        \n",
    "        self.linear = Linear(channels_in * k ** 2, channels_out)\n",
    "        self.k = k\n",
    "        self.padding = 0\n",
    "        self.channels_out = channels_out\n",
    "        \n",
    "        self.x = None\n",
    "        self.out_shape = None\n",
    "        self.channels_in = channels_in\n",
    "        \n",
    "    def forward(self, x):\n",
    "        assert x.shape[1] == self.channels_in\n",
    "        assert len(x.shape) == 4\n",
    "        assert x.shape[-2] == x.shape[-1]\n",
    "        \n",
    "        self.x = x\n",
    "        s = x.shape\n",
    "        k = self.k        \n",
    "        slices = []\n",
    "        for i in range(s[-1] - k + 1):\n",
    "            for j in range(s[-1] - k + 1):\n",
    "                slices.append(np.reshape(x[..., i: i + k, j: j + k], [s[0], -1]))\n",
    "        stacked = np.stack(slices, axis=1)   \n",
    "        a = self.linear.forward(stacked)\n",
    "        d_out = s[-1] + 2 * self.padding - (k - 1)\n",
    "        a = np.reshape(a, [s[0], self.channels_out, d_out, d_out])\n",
    "        self.out_shape = a.shape\n",
    "        return a\n",
    "    \n",
    "    def backward(self, d):\n",
    "        assert d.shape == self.out_shape\n",
    "        k = self.k\n",
    "        d = d.reshape(self.linear.out_shape)\n",
    "        d = self.linear.backward(d)\n",
    "        \n",
    "                \n",
    "        d_dx = np.zeros(self.x.shape)\n",
    "\n",
    "        for i in range(d_dx.shape[-2] - k + 1):\n",
    "            for j in range(d_dx.shape[-1] - k + 1):        \n",
    "                col_ix = i * (d_dx.shape[-2] - k + 1) + j\n",
    "                d_dx[:, :, i: i + k, j: j + k] += np.reshape(d[:, col_ix], [d_dx.shape[0], d_dx.shape[1], k, k])\n",
    "        \n",
    "        return d_dx\n",
    "    \n",
    "    \n",
    "    def step(self, lr):\n",
    "        self.linear.step(lr)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear:\n",
    "    def __init__(self, n_in, n_out):\n",
    "        self.w = np.random.randn(n_in, n_out) * 0.1\n",
    "#         self.w = np.linspace(-0.5, 1, n_in * n_out).reshape([n_in, n_out])\n",
    "        self.b = np.zeros([n_out])\n",
    "        \n",
    "        self.dw = None\n",
    "        self.db = None\n",
    "        \n",
    "        self.x = None\n",
    "        \n",
    "        self.out_shape = None\n",
    "    \n",
    "    def forward(self, x):\n",
    "        assert len(x.shape) == 3\n",
    "        assert len(self.w.shape) == 2\n",
    "        assert len(self.b.shape) == 1\n",
    "\n",
    "        self.x = x\n",
    "        y = x @ self.w + self.b\n",
    "        self.out_shape = y.shape\n",
    "        return y\n",
    "    \n",
    "    def backward(self, d):\n",
    "        assert d.shape == self.out_shape, (d.shape, self.out_shape)\n",
    "        \n",
    "        self.db = np.sum(d, axis=(0, 1))\n",
    "        assert self.db.shape == self.b.shape\n",
    "        \n",
    "        d_dw = np.zeros([*d.shape, *self.w.shape])\n",
    "        \n",
    "        for i in range(d.shape[1]):\n",
    "            for j in range(d.shape[2]):\n",
    "                d_dw[:, i, j, :, j] = self.x[:, i, :]\n",
    "        \n",
    "        self.dw = np.tensordot(d, d_dw, axes=3)\n",
    "        assert self.dw.shape == self.w.shape\n",
    "        \n",
    "        d_dx = np.zeros([*d.shape, *self.x.shape[1:]])\n",
    "        for i in range(d.shape[1]):\n",
    "            for j in range(d.shape[2]):\n",
    "                d_dx[:, i, j, i, :] = self.w[:, j]\n",
    "\n",
    "        d = np.tensordot(d, d_dx, axes=([1, 2], [1, 2])).sum(1) / len(d)\n",
    "        assert d.shape == self.x.shape\n",
    "        return d\n",
    "\n",
    "    def step(self, lr):\n",
    "        self.w = self.w - lr * self.dw\n",
    "        self.b = self.b - lr * self.db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU:\n",
    "    def __init__(self):\n",
    "        self.a = None\n",
    "        \n",
    "    def forward(self, x):\n",
    "        self.a = np.maximum(x, 0)\n",
    "        return self.a\n",
    "    \n",
    "    def backward(self, d):\n",
    "        return d * (self.a != 0).astype(np.float32)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Softmax:\n",
    "    def __init__(self):\n",
    "        self.a = None\n",
    "        \n",
    "    def forward(self, x):\n",
    "        assert len(x.shape) == 3\n",
    "        x = x - np.max(x, axis=-1, keepdims=True)\n",
    "        self.a = np.exp(x) / np.sum(np.exp(x), keepdims=True, axis=-1)\n",
    "        return self.a\n",
    "    def backward(self, d):\n",
    "        \n",
    "        diag = np.stack([np.diag(self.a[i, 0]) for i in range(len(self.a))])\n",
    "        op = np.stack([np.outer(self.a[i, 0], self.a[i, 0]) for i in range(len(self.a))])\n",
    "        J = diag - op\n",
    "        \n",
    "        return d @ J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MeanSquaredError:\n",
    "    def __init__(self):\n",
    "        self.y_ = None\n",
    "    def forward(self, y_, y):\n",
    "        assert y_.shape == y.shape, (y_.shape, y.shape)\n",
    "        self.y_ = y_\n",
    "        self.y = y\n",
    "        l = 0.5 * np.sum(np.square(y - y_)) / len(y_)\n",
    "        return l\n",
    "    \n",
    "    def backward(self):\n",
    "        d = -(self.y - self.y_) / len(self.y_)\n",
    "        return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossEntropy:\n",
    "    def forward(self, y_, y):\n",
    "        self.y_ = y_\n",
    "        self.y = y\n",
    "        \n",
    "        assert y_.shape == y.shape\n",
    "        \n",
    "        l = - np.sum(y * np.log(y_))\n",
    "        l /= len(y)\n",
    "        return y_, l\n",
    "    \n",
    "    def backward(self):\n",
    "        y_ = self.y_\n",
    "        y = self.y\n",
    "        \n",
    "        assert y_.shape == y.shape\n",
    "        d = (- y / y_) / len(y_)\n",
    "        return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN:\n",
    "    def __init__(self):\n",
    "        self.conv1 = Conv(1, 4, 3)\n",
    "        self.relu1 = ReLU()\n",
    "        self.conv2 = Conv(4, 8, 5)\n",
    "        self.relu2 = ReLU()\n",
    "        self.lin1 = Linear(32, 10)\n",
    "        self.softmax = Softmax()\n",
    "        # self.mse = MeanSquaredError()\n",
    "        self.ce = CrossEntropy()\n",
    "                \n",
    "    def forward(self, x, t):\n",
    "        z1 = self.conv1.forward(x)\n",
    "        a1 = self.relu1.forward(z1)\n",
    "\n",
    "        z2 = self.conv2.forward(a1)\n",
    "        a2 = self.relu2.forward(z2)\n",
    "        \n",
    "        a2_r = a2.reshape(a2.shape[0], 1, 32)\n",
    "        \n",
    "        a3 = self.lin1.forward(a2_r)\n",
    "\n",
    "        y = self.softmax.forward(a3)\n",
    "                \n",
    "        y, loss = self.ce.forward(y, t)       \n",
    "        \n",
    "        return y, loss\n",
    "    \n",
    "    def backward(self):\n",
    "        dl_dy = self.ce.backward()\n",
    "        dy_da3 = self.softmax.backward(dl_dy)\n",
    "\n",
    "        dl_da2_r = self.lin1.backward(dy_da3)\n",
    "        dl_da2 = dl_da2_r.reshape(dl_da2_r.shape[0], 8, 2, 2)\n",
    "        dl_dz2 = self.relu2.backward(dl_da2)\n",
    "\n",
    "        dl_da1 = self.conv2.backward(dl_dz2)\n",
    "        dl_dz1 = self.relu1.backward(dl_da1)\n",
    "        dl_dx = self.conv1.backward(dl_dz1)\n",
    "\n",
    "    \n",
    "    def step(self, lr):\n",
    "        self.conv1.step(lr)\n",
    "        self.conv2.step(lr)\n",
    "        self.lin1.step(lr)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = CNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss 2.2912227009921136\n",
      "Accuracy: 0.09375\n",
      "Loss 2.242544418623601\n",
      "Accuracy: 0.2734375\n",
      "Loss 2.076377864154841\n",
      "Accuracy: 0.4375\n",
      "Loss 1.524691886054848\n",
      "Accuracy: 0.640625\n",
      "Loss 0.8617892385286434\n",
      "Accuracy: 0.7265625\n",
      "Loss 0.5106373478791786\n",
      "Accuracy: 0.875\n",
      "Loss 0.33728525980108137\n",
      "Accuracy: 0.9296875\n",
      "Loss 0.27180674821441697\n",
      "Accuracy: 0.9453125\n",
      "Loss 0.2478048446905387\n",
      "Accuracy: 0.9453125\n",
      "Loss 0.23448731966680078\n",
      "Accuracy: 0.9296875\n",
      "Loss 0.22317860564383313\n",
      "Accuracy: 0.921875\n",
      "Loss 0.21297551286721356\n",
      "Accuracy: 0.921875\n",
      "Loss 0.20182121189628613\n",
      "Accuracy: 0.9140625\n",
      "Loss 0.19109531990835396\n",
      "Accuracy: 0.921875\n",
      "Loss 0.18068391817910906\n",
      "Accuracy: 0.9140625\n",
      "Loss 0.17129105213208934\n",
      "Accuracy: 0.921875\n"
     ]
    }
   ],
   "source": [
    "for i in range(16):\n",
    "    for j in range(0, len(x_n), batch_size):\n",
    "        x_batch = x_n[j: j + batch_size]\n",
    "        t_batch = t_n[j: j + batch_size]\n",
    "        y_batch, loss = cnn.forward(x_batch, t_batch)\n",
    "        cnn.backward()\n",
    "        cnn.step(lr=0.1)\n",
    "        if i % 1 == 0 and j == 0:\n",
    "#             print(j)\n",
    "            print('Loss', loss)\n",
    "            print('Accuracy:', (np.argmax(y_batch, axis=-1) == np.argmax(t_batch, axis=-1)).sum() / len(y_batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
